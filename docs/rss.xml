<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Brue Leo's Blog</title><link>https://Ethan-a2.github.io</link><description>Freedom is not free.</description><copyright>Brue Leo's Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/61342241?v=4&amp;size=64</url><title>avatar</title><link>https://Ethan-a2.github.io</link></image><lastBuildDate>Sun, 07 Sep 2025 09:09:36 +0000</lastBuildDate><managingEditor>Brue Leo's Blog</managingEditor><ttl>60</ttl><webMaster>Brue Leo's Blog</webMaster><item><title>d2ltvmç¼–è¯‘</title><link>https://Ethan-a2.github.io/post/d2ltvm-bian-yi.html</link><description>
# ç¯å¢ƒ
- tvm-0.19 + numpy  &lt; 1.24
- ubuntu 22.04
- llvm-15
- å› ä¸ºä¾èµ–mxnetï¼Œä½†æ˜¯mxnet 2022åä¸å†æ›´æ–°ï¼Œæ‰€ä»¥numpyè¦é™çº§åˆ°1.24ä»¥å‰çš„ç‰ˆæœ¬
- TVM 0.20çš„ç‰ˆæœ¬æœ‰äº†æ¯”è¾ƒå¤§çš„æ›´æ–°ï¼Œæ‰€ä»¥è¿™é‡Œé€‚é…äº†0.19çš„ç‰ˆæœ¬
	- https://github.com/apache/tvm/issues/17914
- LLVMæ›´é«˜çš„ç‰ˆæœ¬åº”è¯¥ä¹Ÿæ˜¯ï¼Œä½†æ˜¯ubuntu22.04å®‰è£…llvm-15æ¯”è¾ƒæ–¹ä¾¿ï¼Œå°±å…ˆç”¨äº†è¿™ä¸ªç‰ˆæœ¬


# ç¼–è¯‘é€šè¿‡çš„ç‰ˆæœ¬
- https://github.com/Ethan-a2/d2ltvm
- é€‚é…äº†TVM0.19ä¹‹åçš„ç‰ˆæœ¬ï¼Œå‰4ç« å¯ä»¥æ­£å¸¸ç¼–è¯‘è¿è¡Œ

# å„ç« æ€ç»´å¯¼å›¾


```mermaid
mindmap
  root((Getting Started))
    Installation
      Python env
      TVM build (LLVM, CUDA)
      MXNet
    Vector Add
      NumPy impl
      TVM compute
      Schedule
      Build &amp; Run
    Neural Network Inference
      Pretrained model (ResNet18)
      Relay IR
      Compilation (LLVM)
      Save/Load Module
    Remote Execution
      RPC server
      Cross-compilation
      Run on remote device
```





```mermaid
mindmap
  root((Expressions for Operators))
    Data Types
      float32
      int8
      cast/astype
    Shapes
      te.var
      multi-dim tensors
    Index &amp; Shape Expressions
      Transpose
      Reshape
      Slicing
    Reduction
      te.reduce_axis
      te.sum, te.max, te.min
      comm_reducer
    Conditional
      if_then_else
    Truth Values
      all
      any
```



```mermaid
mindmap
  root((Common Operators))
    Broadcast Add
    MatMul
    Conv2D
    Depthwise Conv
    Pooling
      MaxPool
      AvgPool
    BatchNorm
```



```mermaid
mindmap
  root((CPU Operator Optimizations))
    CPU Architecture
      Cache
      SIMD
      Pipeline
    Function Call Overhead
    Vector Add
    Broadcast Add
    MatMul
      Loop blocking
      Vectorization
    Convolution
      Packed layout
    Pooling
    BatchNorm
```



```mermaid
mindmap
  root((GPU Operator Optimizations))
    GPU Architecture
      Warp
      SM
      Shared Memory
    Vector Add
    Broadcast Add
    MatMul
      Thread tiling
      Shared memory
    Conv2D
    Depthwise Conv
    Pooling
    BatchNorm
```



# TVMç¼–è¯‘

```
sudo apt-get install -y -q \
        git \
        cmake \
        build-essential \
        libtinfo-dev \
        libffi-dev \
        zlib1g-dev \
        llvm-15-dev \
        libclang-15-dev \
        ninja-build

sudo apt install nvidia-cuda-toolkit

git clone --recursive https://github.com/apache/tvm tvm-src
mkdir build; cd build

cp ../cmake/config.cmake .
echo 'set(USE_LLVM \'llvm-config-15 --ignore-libllvm --link-static\')' &gt;&gt; config.cmake
sed -i 's/# set(USE_CUDA OFF)/set(USE_CUDA ON)/g' config.cmake
set(USE_GTEST OFF)

cmake ../
cmake --build . --parallel $(nproc)
```


# d2ltvmç›¸å…³æ‰‹å†Œ
- https://tvm.d2l.ai/d2l-tvm.pdf
- https://github.com/d2l-ai/d2l-tvm
- http://tvm.d2l.ai/d2l-tvm.zip
- https://tvm.d2l.ai/


# ç»“è®º
- æä¾›ä¸€ç§æ–¹å¼ï¼šä¸ç”¨æ‰‹å·¥å†™åº•å±‚ CUDA/C++ å†…æ ¸ï¼Œè€Œæ˜¯é€šè¿‡ç¼–è¯‘å™¨æ¡†æ¶ç»Ÿä¸€è¡¨è¾¾å’Œä¼˜åŒ–ç®—å­/æ¨¡å‹ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/d2ltvm-bian-yi.html</guid><pubDate>Sun, 07 Sep 2025 09:09:08 +0000</pubDate></item><item><title>å°ç±³15 NPUéƒ¨ç½²Llama-3.2-1B-Instruct</title><link>https://Ethan-a2.github.io/post/xiao-mi-15%20NPU-bu-shu-Llama-3.2-1B-Instruct.html</link><description># å‡†å¤‡
- https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
- æ˜¾å­˜è‡³å°‘éœ€è¦12Gã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/xiao-mi-15%20NPU-bu-shu-Llama-3.2-1B-Instruct.html</guid><pubDate>Sun, 03 Aug 2025 11:06:50 +0000</pubDate></item><item><title>HexagonNPUè¿è¡Œllama.cpp</title><link>https://Ethan-a2.github.io/post/HexagonNPU-yun-xing-llama.cpp.html</link><description>æœ¬æ–‡å°†è¯¦ç»†ä»‹yç»å¦‚ä½•åœ¨ Android è®¾å¤‡ä¸Šï¼Œåˆ©ç”¨ `llama.cpp` æ¡†æ¶ï¼Œç»“åˆé«˜é€šç¥ç»ç½‘ç»œå¤„ç†å•å…ƒ (QNN) çš„ç¡¬ä»¶åŠ é€Ÿèƒ½åŠ›ï¼Œéƒ¨ç½²å¹¶è¿è¡Œ Qwen 1.5 æ¨¡å‹ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/HexagonNPU-yun-xing-llama.cpp.html</guid><pubDate>Tue, 22 Jul 2025 13:01:57 +0000</pubDate></item><item><title>Redmi K70 Proè¿è¡ŒLlama 1.1B</title><link>https://Ethan-a2.github.io/post/Redmi%20K70%20Pro-yun-xing-Llama%201.1B.html</link><description>
# å‰è¨€

éšç€ç§»åŠ¨è®¾å¤‡ç¡¬ä»¶æ€§èƒ½çš„é£é€Ÿå‘å±•ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²åˆ°ç«¯ä¾§è®¾å¤‡ä¸Šè¿è¡Œï¼Œå®ç°ä½å»¶è¿Ÿã€ä¿æŠ¤ç”¨æˆ·éšç§çš„AIæ¨ç†ï¼Œæ­£æˆä¸ºä¸€ä¸ªçƒ­é—¨æ–¹å‘ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Redmi%20K70%20Pro-yun-xing-Llama%201.1B.html</guid><pubDate>Thu, 17 Jul 2025 15:31:10 +0000</pubDate></item><item><title>å°ç±³15Ultraè¿è¡ŒQwen2-7B-Instruct</title><link>https://Ethan-a2.github.io/post/xiao-mi-15Ultra-yun-xing-Qwen2-7B-Instruct.html</link><description>
# environment
- Operating System: Ubuntu 22.04
- QNN SDK Version: 2.36.0.250627
- devicesï¼šxiaomi 15Ultra 16Gå†…å­˜, SnapdragonÂ® 8 Elite Mobile
- genie-t2t-run

- æ³¨æ„ï¼Œ[Qwen2-7B-Instruct - Qualcomm AI Hub](https://aihub.qualcomm.com/models/qwen2_7b_instruct)ç›®å‰åªæ”¯æŒSnapdragonÂ® 8 Elite Mobileã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/xiao-mi-15Ultra-yun-xing-Qwen2-7B-Instruct.html</guid><pubDate>Tue, 15 Jul 2025 14:03:37 +0000</pubDate></item><item><title>0713.Xiaomi14Ultraé€šè¿‡QNNè¿è¡ŒQwen</title><link>https://Ethan-a2.github.io/post/0713.Xiaomi14Ultra-tong-guo-QNN-yun-xing-Qwen.html</link><description>
# environment
- Operating System: Ubuntu 22.04
- QNN SDK Version: 2.31.0.250130
- Hexagon SDK Version: 5.5.5.0
- Android NDK Version: android-ndk-r26d
- devicesï¼šxiaomi 14Ultra, 8gen3
- mllm



# build
```
cd /opt/qcom/Hexagon_SDK
ln -s 5.5.5.0 HexagonSDK

export QNN_SDK_ROOT=/opt/qcom/aistack/qairt/2.31.0.250130
source $QNN_SDK_ROOT/bin/envsetup.sh
source /opt/qcom/Hexagon_SDK/5.5.5.0/setup_sdk_env.source

git clone https://github.com/UbiquitousLearning/mllm.git

CMakeLists.txt ä¸­gtestç›¸å…³çš„æ³¨é‡Šæ‰



cd src/backends/qnn
ln -s $QNN_SDK_ROOT sdk

cd src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/
make htp_aarch64 &amp;&amp; make htp_v75

./build_android_qnn.sh
./run_qwen_qnn.sh

è¦åŠ ä¸‹è¿™ä¸¤å¥:
adb shell mkdir -p /data/local/tmp/mllm/bin
adb push ../vocab/qwen_merges.txt /data/local/tmp/mllm/vocab/
```


# result
```
./run_qwen_qnn.sh 
../vocab/qwen_vocab.mllm: 1 file pushed, 0 skipped. 95.0 MB/s (3189278 bytes in 0.032s)
../vocab/qwen_merges.txt: 1 file pushed, 0 skipped. 81.8 MB/s (1671839 bytes in 0.019s)
qwen-1.5-1.8b-chat-int8 file already exists
qwen-1.5-1.8b-chat-q4k.mllm file already exists
QNN_SDK_ROOT is set to /opt/qcom/aistack/qairt/2.31.0.250130
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtp.so: 1 file pushed, 0 skipped. 53.6 MB/s (2060792 bytes in 0.037s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpV75Stub.so: 1 file pushed, 0 skipped. 22.9 MB/s (453640 bytes in 0.019s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpPrepare.so: 1 file pushed, 0 skipped. 33.4 MB/s (55080032 bytes in 1.575s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpProfilingReader.so: 1 file pushed, 0 skipped. 113.0 MB/s (579072 bytes in 0.005s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpOptraceProfilingReader.so: 1 file pushed, 0 skipped. 153.1 MB/s (3073968 bytes in 0.019s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpV75CalculatorStub.so: 1 file pushed, 0 skipped. 0.4 MB/s (6472 bytes in 0.015s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/hexagon-v75/unsigned/libQnnHtpV75Skel.so: 1 file pushed, 0 skipped. 89.1 MB/s (8485728 bytes in 0.091s)
../src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/build/aarch64-android/libQnnLLaMAPackage.so: 1 file pushed, 0 skipped. 300.3 MB/s (1479704 bytes in 0.005s)
../src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/build/hexagon-v75/libQnnLLaMAPackage.so: 1 file pushed, 0 skipped. 177.0 MB/s (818800 bytes in 0.004s)
../bin-arm/demo_qwen_npu: 1 file pushed, 0 skipped. 72.1 MB/s (93400608 bytes in 1.235s)
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:118] Backend: libQnnHtp.so
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:143] Backend        build version: v2.31.0.250130151446_114721
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:166] Initialize Backend Returned Status = 0
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:636] Registered Op Package: libQnnLLaMAPackage_CPU.so and interface provider: LLaMAPackageInterfaceProvider
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:636] Registered Op Package: libQnnLLaMAPackage_HTP.so and interface provider: LLaMAPackageInterfaceProvider
Warmup finished.
[Q] 'Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. These models are trained on vast amounts of data, enabling them to perform a wide range of tasks, from answering questions and summarizing text to generating creative content and engaging in conversational dialogue. LLMs like GPT-3 and GPT-4, developed by OpenAI, have set new benchmarks in natural language processing by leveraging deep learning architectures, particularly transformer models, which excel at capturing context and relationships within text. The scalability and versatility of LLMs make them invaluable tools for applications in education, customer service, content creation, and more. However, their deployment also raises ethical considerations, including issues of bias, misinformation, and the potential for misuse. As the field continues to evolve, ongoing research and responsible deployment strategies are essential to harnessing the full potential of these powerful AI systems while mitigating their risks.'
Generate a title based on the above text.
[A] Exploring the Advantages and Ethical Considerations of Large Language Models (LLMs): A Comprehensive OverviewSegmentation fault 
```


# issue
- é€Ÿåº¦å¾ˆæ…¢ï¼Œä¸äº†è§£åŸå› 


# references
- [mllm/src/backends/qnn/README.md at main Â· UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm/blob/main/src/backends/qnn/README.md)
- 
ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/0713.Xiaomi14Ultra-tong-guo-QNN-yun-xing-Qwen.html</guid><pubDate>Sun, 13 Jul 2025 07:04:58 +0000</pubDate></item><item><title>Android K70 Pro llama.cpp Gemma-3n</title><link>https://Ethan-a2.github.io/post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html</link><description># è®¾å¤‡
- K70 Pro Adreno 750 (Snapdragon 8 Gen 3)
- NDK26
- Adrenoä½¿ç”¨openclç¼–è¯‘


# æ¨¡å‹
- gemma-3n-E2B-it-UD-Q4_K_XL.gguf


# Android CPUä¸Šè¿è¡Œ
```
ndk-r26d:
cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-28 -DCMAKE_C_FLAGS='-march=armv8.7a' -DCMAKE_CXX_FLAGS='-march=armv8.7a' -DGGML_OPENMP=OFF  -DGGML_LLAMAFILE=OFF -DLLAMA_CURL=OFF -B build-android
cmake --build build -j16
cmake --install build-android --prefix ../install

cd ../install
adb shell 'mkdir /data/local/tmp/llama.cpp'
adb push install/. /data/local/tmp/llama.cpp/
adb push {model}.gguf /data/local/tmp/llama.cpp/
adb shell 'ls /data/local/tmp/llama.cpp'
adb shell
cd /data/local/tmp/llama.cpp/lib
export LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/lib:$LD_LIBRARY_PATH
./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose
```


# Adrenoä¸Šè¿è¡Œ
```
Adreno:

Install OpenCL Headers and Library
git clone https://github.com/KhronosGroup/OpenCL-Headers &amp;&amp; \
cd OpenCL-Headers &amp;&amp; \
cp -rf ./CL/ $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include

git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader &amp;&amp; \
cd OpenCL-ICD-Loader &amp;&amp; \
mkdir build_ndk26 &amp;&amp; cd build_ndk26
cmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=24 -DANDROID_STL=c++_shared
cp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android

Build llama.cpp:
cmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a  -DANDROID_PLATFORM=android-28  -DBUILD_SHARED_LIBS=OFF -DGGML_OPENCL=ON  -DLLAMA_CURL=OFF
ninja -j16
DESTDIR=../install-opencl/ ninja install


cd ../install-opencl/usr/loca/
adb shell 'mkdir /data/local/tmp/llama.cpp-opencl'
adb push . /data/local/tmp/llama.cpp-opencl/
adb push {model}.gguf /data/local/tmp/llama.cpp-opencl/
adb shell 'ls /data/local/tmp/llama.cpp-opencl'
adb shell
cd /data/local/tmp/llama.cpp-opencl/lib
export LD_LIBRARY_PATH=/data/local/tmp/llama.cpp-opencl/lib:$LD_LIBRARY_PATH
./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose
```


# Reference
- https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md
- https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md
- [google-ai-edge](https://github.com/google-ai-edge)

# issue
- ç”¨CPUåŸºæœ¬ä¸Googleå®˜æ–¹çš„galleryæ€§èƒ½ä¸€è‡´ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html</guid><pubDate>Sun, 29 Jun 2025 09:16:17 +0000</pubDate></item><item><title>minimind.inference</title><link>https://Ethan-a2.github.io/post/minimind.inference.html</link><description>
# download
```
git clone https://github.com/jingyaogong/minimind.git
huggingface-cli download --local-dir MiniMind2
```

# test
```
python eval_model.py --load 1 --model_mode 2

cd scripts
streamlit run web_demo.py

```

# ä½¿ç”¨llama.cppæ¨ç†
åœ¨llama.cppä¸­æ‰“ä¸Šå¦‚ä¸‹patch:
```
diff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py
index bf6bc683..5ea853b6 100755
--- a/convert_hf_to_gguf.py
+++ b/convert_hf_to_gguf.py
@@ -808,7 +808,8 @@ class TextModel(ModelBase):
             logger.warning(f'** chkhsh:  {chkhsh}')
             logger.warning('**************************************************************************************')
             logger.warning('\n')
-            raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
+            res = 'smollm'
+            #raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
 
         logger.debug(f'tokenizer.ggml.pre: {repr(res)}')
         logger.debug(f'chkhsh: {chkhsh}')
```

```
python convert_hf_to_gguf.py ../minimind/MiniMind2/

./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M

./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml

```

# ä½¿ç”¨ollamaéƒ¨ç½²
æ–°å»ºminimind.modelfileï¼š
```
FROM ./MiniMind2-109M-F16.gguf
TEMPLATE '''{{ if .System }}&lt;|im_start|&gt;system
{{ .System }}&lt;|im_end|&gt;
{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user
{{ .Prompt }}&lt;|im_end|&gt;
{{ end }}&lt;|im_start|&gt;assistant
'''
```

åŠ è½½æ¨¡å‹å¹¶å‘½åä¸ºminimind2
```
ollama create -f minimind.modelfile minimind2
```

å¯åŠ¨æ¨ç†

```
ollama run minimind2
```

# reference
[jingyaogong/minimind: ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!](https://github.com/jingyaogong/minimind#vllm%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1)
ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/minimind.inference.html</guid><pubDate>Sun, 18 May 2025 04:21:37 +0000</pubDate></item><item><title>llama3.cppåŸºäºNVIDIA1060è¿›è¡Œå›¾ç‰‡å¯¹è¯</title><link>https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</link><description>
# ç¯å¢ƒ
æ˜¾å¡ï¼šNVIDIA GeForce GTX 1060
å›¾ç‰‡å¯¹è¯æ¨¡å‹ï¼šSmolVLM-500M-Instruct



# ç¼–è¯‘

```
sudo apt-get install libssl-dev libcurl4-openssl-dev

git clone https://github.com/ggml-org/llama.cpp.git
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=61
make -j16
```


åœ¨è¿™é‡Œ[Ollama GPUæ”¯æŒ - Nvidiaå’ŒAMD GPUå…¼å®¹æ€§ | LlamaFactory | LlamaFactory](https://www.llamafactory.cn/ollama-docs/gpu.html#google_vignette)æ‰¾åˆ°1060çš„æ¶æ„æ˜¯6.1ï¼Œç¼–è¯‘æ—¶æŒ‡å®š-DCMAKE_CUDA_ARCHITECTURES=61


# æ¨¡å‹GGUFä¸‹è½½
```
export HF_HOME=/media/dataset/hf
export HF_ENDPOINT=https://hf-mirror.com
export HUGGINGFACE_TOKEN=xxx

huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF
huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF --local-dir .
```


# è¿è¡Œllama3.cpp
```
export PATH'$PATH:/path/to/llama.cpp/build/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/llama.cpp/build/bin

llama-server -m ./SmolVLM-500M-Instruct-Q8_0.gguf --mmproj mmproj-SmolVLM-500M-Instruct-Q8_0.gguf --host 0.0.0.0 --port 8080 -ngl 100
```


è¾“å‡ºæ—¥å¿—:
```
User: Hello&lt;end_of_utterance&gt;
Assistant: Hi there&lt;end_of_utterance&gt;
User: How are you?&lt;end_of_utterance&gt;
Assistant:'
main: server is listening on http://0.0.0.0:8080 - starting the main loop
srv  update_slots: all slots are idle
```


- ç”¨æµè§ˆå™¨æ‰“å¼€ç½‘å€ï¼Œæ¯”å¦‚ 192.168.3.100:1080ï¼Œå³å¯ä»¥è¿›è¡Œå›¾ç‰‡å¯¹è¯ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</guid><pubDate>Wed, 14 May 2025 15:00:48 +0000</pubDate></item><item><title>Qwen3æ€§èƒ½è¯„æµ‹-evalscope</title><link>https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</link><description>



# ç¯å¢ƒå‡†å¤‡ 
```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1060        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   62C    P8               3W /  78W |   2426MiB /  6144MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                            4MiB |
|    0   N/A  N/A      3029      C   /usr/local/bin/ollama                      2418MiB |
+---------------------------------------------------------------------------------------+
```


```
ollama run qwen3:1.7b
pip install evalscope
```


# è¿è¡Œ
```
evalscope perf \
    --model qwen3:1.7b \
    --url 'http://192.168.1.3:11434/v1/chat/completions' \
    --parallel 5 \
    --number 20 \
    --api openai \
    --dataset openqa \
    --stream
```


# ç»“æœ
```
'Time taken for tests (s)': 288.8133,
'Number of concurrency': 5,
'Total requests': 10,
'Succeed requests': 10,
'Failed requests': 0,
'Output token throughput (tok/s)': 36.605,
'Total token throughput (tok/s)': 37.5848,
'Request throughput (req/s)': 0.0346,
'Average latency (s)': 112.7879,
'Average time to first token (s)': 22.1013,
'Average time per output token (s)': 0.0865,
'Average input tokens per request': 28.3,
'Average output tokens per request': 1057.2,
'Average package latency (s)': 0.0865,
'Average package per request': 1048.3
```






# reference
- [Best Practices for Evaluating the Qwen3 Model | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- [é‡åŒ–æ¨¡å‹æ•ˆæœè¯„ä¼° - Qwen](https://qwen.readthedocs.io/zh-cn/latest/getting_started/quantization_benchmark.html)
- [How Smart is Your AI? Full Assessment of IQ and EQ! | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html)
- ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</guid><pubDate>Sun, 11 May 2025 15:29:14 +0000</pubDate></item><item><title>MNN-LLM</title><link>https://Ethan-a2.github.io/post/MNN-LLM.html</link><description>

```
git lfs install
git clone https://www.modelscope.cn/qwen/Qwen2-0.5B-Instruct.git

æˆ–è€…
huggingface-cli download taobao-mnn/Qwen2-0.5B-Instruct-MNN --local-dir .//Qwen2-0.5B-Instruct-MNN
```

# export

```
pip3 install torch torchvision torchaudio --force-reinstall
pip install numpy==1.26.4
python llmexport.py --path /media/code/mnn/Qwen2-0.5B-Instruct --export mnn



```



# build
```
cmake -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON -DMNN_BUILD_BENCHMARK=true -DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true ..

make -j16
```



# run
```
./llm_demo /media/code/mnn/MNN/transformers/llm/export/model/config.json
```


# issue
èŠå¤©æ•ˆæœå¾ˆå·®ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-LLM.html</guid><pubDate>Sat, 10 May 2025 08:01:42 +0000</pubDate></item><item><title>MNN-demo</title><link>https://Ethan-a2.github.io/post/MNN-demo.html</link><description>
# å®ä¾‹åˆ†å‰²
è½¬æ¢æ—¶éœ€è¦æ·»åŠ --keepInputFormat falseå‚æ•°ï¼Œè½¬ä¸ºHCHWçš„æ ¼å¼ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-demo.html</guid><pubDate>Sat, 10 May 2025 05:29:34 +0000</pubDate></item><item><title>MNN multiPose demo</title><link>https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</link><description>
[MNN/demo/exec at master Â· Ethan-a2/MNN](https://github.com/Ethan-a2/MNN/tree/master/demo/exec)

# build
```
cd path/to/MNN
mkdir build
cd build
cmake -G 'NMake Makefiles' -DCMAKE_BUILD_TYPE=Release -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON ..
make
```


# convert
```
./MNNConvert -f TF --modelFile model-mobilenet_v1_075.pb  --MNNModel model-mobilenet_v1_075.mnn --bizCode biz
```

# excute
```
./multiPose.out model-mobilenet_v1_075.mnn  multipose_input.png  pose.png
```


ç¤ºä¾‹å›¾ç‰‡åœ¨è¿™ä¸ªè·¯å¾„:
```
../_static/images/start/multipose_input.png
```


å‚è€ƒ:
https://mnn-docs.readthedocs.io/en/2.9.6/_sources/start/demo.md.txt

# æŠ¥é”™ä¿¡æ¯
```
CPU Group: [ 2  0  3  1 ], 800000 - 3500000
The device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0, sme2: 0
        **Tensor shape**: 1, 225, 3, 225, 
Error for compute convolution shape, inputCount:3, outputCount:24, KH:3, KW:3, group:1
inputChannel: 225, batch:1, width:225, height:3. Input data channel may be mismatch with filter channel count
Compute Shape Error for MobilenetV1/add
Invalid Tensor, the session may not be ready
Can't run session because not resized
main, 381, cost time: 0.004000 ms
main, 405, cost time: 0.001000 ms
```


# è§£å†³åŠæ³•ä¸€
å°†è¾“å…¥çš„å›¾åƒè½¬æ¢ä¸ºtfæ¨¡å—çš„NHWCçš„æ ¼å¼ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</guid><pubDate>Sat, 10 May 2025 02:48:11 +0000</pubDate></item><item><title>dockerå®‰è£…æµè§ˆå™¨</title><link>https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</link><description>docker-compose.yml:
```
services:
  firefox:
    image: jlesage/firefox
    container_name: firefox
    ports:
      - '5800:5800' # æ ¼å¼: 'å®¿ä¸»æœºç«¯å£:å®¹å™¨ç«¯å£'
    volumes:
      - /media/docker/firefox/config:/config:rw 
      - /media/docker/firefox/downloads:/downloads:rw
    restart: unless-stopped # å¯é€‰ï¼šæ·»åŠ é‡å¯ç­–ç•¥ï¼Œè¿™æ ·å®¹å™¨æ„å¤–é€€å‡ºæˆ– Docker é‡å¯æ—¶ä¼šè‡ªåŠ¨å¯åŠ¨
```


# references
- https://github.com/jlesage/docker-firefox
- ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</guid><pubDate>Sat, 03 May 2025 06:19:22 +0000</pubDate></item><item><title>ipv6å†…ç½‘ç©¿é€</title><link>https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</link><description>
# æ­¥éª¤
1. å…‰çŒ«æ”¹æ¡¥æ¥ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</guid><pubDate>Sat, 03 May 2025 03:36:05 +0000</pubDate></item><item><title>å›½å†…githubåŠ é€Ÿ</title><link>https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</link><description>å¾®è½¯åº”ç”¨å•†åº—ä¸‹è½½Watt ToolKit
[Watt Toolkit - Free download and install on Windows | Microsoft Store](https://apps.microsoft.com/detail/9mtcfhs560ng?hl=en-US&amp;gl=CN)

![Image](https://github.com/user-attachments/assets/f980382b-eba6-43f4-a747-19fda38544b5)

ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</guid><pubDate>Wed, 30 Apr 2025 21:51:08 +0000</pubDate></item><item><title>githubå¿«é€Ÿæ­å»ºåšå®¢</title><link>https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</link><description>

# æ³¨æ„
- æissueçš„æ—¶å€™å¿…é¡»åŠ ä¸€ä¸ªlabel


# issue
- ä»“åº“åä¸ºxxx.github.ioã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</guid><pubDate>Wed, 30 Apr 2025 21:48:54 +0000</pubDate></item></channel></rss>