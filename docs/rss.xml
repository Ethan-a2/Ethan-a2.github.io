<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Brue Leo's Blog</title><link>https://Ethan-a2.github.io</link><description>Freedom is not free.</description><copyright>Brue Leo's Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/61342241?v=4&amp;size=64</url><title>avatar</title><link>https://Ethan-a2.github.io</link></image><lastBuildDate>Sun, 29 Jun 2025 09:16:51 +0000</lastBuildDate><managingEditor>Brue Leo's Blog</managingEditor><ttl>60</ttl><webMaster>Brue Leo's Blog</webMaster><item><title>Android K70 Pro llama.cpp Gemma-3n</title><link>https://Ethan-a2.github.io/post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html</link><description># 设备
- K70 Pro Adreno 750 (Snapdragon 8 Gen 3)
- NDK26
- Adreno使用opencl编译


# 模型
- gemma-3n-E2B-it-UD-Q4_K_XL.gguf


# Android CPU上运行
```
ndk-r26d:
cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-28 -DCMAKE_C_FLAGS='-march=armv8.7a' -DCMAKE_CXX_FLAGS='-march=armv8.7a' -DGGML_OPENMP=OFF  -DGGML_LLAMAFILE=OFF -DLLAMA_CURL=OFF -B build-android
cmake --build build -j16
cmake --install build-android --prefix ../install

cd ../install
adb shell 'mkdir /data/local/tmp/llama.cpp'
adb push install/. /data/local/tmp/llama.cpp/
adb push {model}.gguf /data/local/tmp/llama.cpp/
adb shell 'ls /data/local/tmp/llama.cpp'
adb shell
cd /data/local/tmp/llama.cpp/lib
export LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/lib:$LD_LIBRARY_PATH
./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose
```


# Adreno上运行
```
Adreno:

Install OpenCL Headers and Library
git clone https://github.com/KhronosGroup/OpenCL-Headers &amp;&amp; \
cd OpenCL-Headers &amp;&amp; \
cp -rf ./CL/ $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include

git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader &amp;&amp; \
cd OpenCL-ICD-Loader &amp;&amp; \
mkdir build_ndk26 &amp;&amp; cd build_ndk26
cmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=24 -DANDROID_STL=c++_shared
cp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android

Build llama.cpp:
cmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a  -DANDROID_PLATFORM=android-28  -DBUILD_SHARED_LIBS=OFF -DGGML_OPENCL=ON  -DLLAMA_CURL=OFF
ninja -j16
DESTDIR=../install-opencl/ ninja install


cd ../install-opencl/usr/loca/
adb shell 'mkdir /data/local/tmp/llama.cpp-opencl'
adb push . /data/local/tmp/llama.cpp-opencl/
adb push {model}.gguf /data/local/tmp/llama.cpp-opencl/
adb shell 'ls /data/local/tmp/llama.cpp-opencl'
adb shell
cd /data/local/tmp/llama.cpp-opencl/lib
export LD_LIBRARY_PATH=/data/local/tmp/llama.cpp-opencl/lib:$LD_LIBRARY_PATH
./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose
```


# Reference
- https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md
- https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md
- [google-ai-edge](https://github.com/google-ai-edge)

# issue
- 用CPU基本与Google官方的gallery性能一致。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html</guid><pubDate>Sun, 29 Jun 2025 09:16:17 +0000</pubDate></item><item><title>minimind.inference</title><link>https://Ethan-a2.github.io/post/minimind.inference.html</link><description>
# download
```
git clone https://github.com/jingyaogong/minimind.git
huggingface-cli download --local-dir MiniMind2
```

# test
```
python eval_model.py --load 1 --model_mode 2

cd scripts
streamlit run web_demo.py

```

# 使用llama.cpp推理
在llama.cpp中打上如下patch:
```
diff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py
index bf6bc683..5ea853b6 100755
--- a/convert_hf_to_gguf.py
+++ b/convert_hf_to_gguf.py
@@ -808,7 +808,8 @@ class TextModel(ModelBase):
             logger.warning(f'** chkhsh:  {chkhsh}')
             logger.warning('**************************************************************************************')
             logger.warning('\n')
-            raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
+            res = 'smollm'
+            #raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
 
         logger.debug(f'tokenizer.ggml.pre: {repr(res)}')
         logger.debug(f'chkhsh: {chkhsh}')
```

```
python convert_hf_to_gguf.py ../minimind/MiniMind2/

./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M

./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml

```

# 使用ollama部署
新建minimind.modelfile：
```
FROM ./MiniMind2-109M-F16.gguf
TEMPLATE '''{{ if .System }}&lt;|im_start|&gt;system
{{ .System }}&lt;|im_end|&gt;
{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user
{{ .Prompt }}&lt;|im_end|&gt;
{{ end }}&lt;|im_start|&gt;assistant
'''
```

加载模型并命名为minimind2
```
ollama create -f minimind.modelfile minimind2
```

启动推理

```
ollama run minimind2
```

# reference
[jingyaogong/minimind: 🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!](https://github.com/jingyaogong/minimind#vllm%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1)
。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/minimind.inference.html</guid><pubDate>Sun, 18 May 2025 04:21:37 +0000</pubDate></item><item><title>llama3.cpp基于NVIDIA1060进行图片对话</title><link>https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</link><description>
# 环境
显卡：NVIDIA GeForce GTX 1060
图片对话模型：SmolVLM-500M-Instruct



# 编译

```
sudo apt-get install libssl-dev libcurl4-openssl-dev

git clone https://github.com/ggml-org/llama.cpp.git
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=61
make -j16
```


在这里[Ollama GPU支持 - Nvidia和AMD GPU兼容性 | LlamaFactory | LlamaFactory](https://www.llamafactory.cn/ollama-docs/gpu.html#google_vignette)找到1060的架构是6.1，编译时指定-DCMAKE_CUDA_ARCHITECTURES=61


# 模型GGUF下载
```
export HF_HOME=/media/dataset/hf
export HF_ENDPOINT=https://hf-mirror.com
export HUGGINGFACE_TOKEN=xxx

huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF
huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF --local-dir .
```


# 运行llama3.cpp
```
export PATH'$PATH:/path/to/llama.cpp/build/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/llama.cpp/build/bin

llama-server -m ./SmolVLM-500M-Instruct-Q8_0.gguf --mmproj mmproj-SmolVLM-500M-Instruct-Q8_0.gguf --host 0.0.0.0 --port 8080 -ngl 100
```


输出日志:
```
User: Hello&lt;end_of_utterance&gt;
Assistant: Hi there&lt;end_of_utterance&gt;
User: How are you?&lt;end_of_utterance&gt;
Assistant:'
main: server is listening on http://0.0.0.0:8080 - starting the main loop
srv  update_slots: all slots are idle
```


- 用浏览器打开网址，比如 192.168.3.100:1080，即可以进行图片对话。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</guid><pubDate>Wed, 14 May 2025 15:00:48 +0000</pubDate></item><item><title>Qwen3性能评测-evalscope</title><link>https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</link><description>



# 环境准备 
```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1060        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   62C    P8               3W /  78W |   2426MiB /  6144MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                            4MiB |
|    0   N/A  N/A      3029      C   /usr/local/bin/ollama                      2418MiB |
+---------------------------------------------------------------------------------------+
```


```
ollama run qwen3:1.7b
pip install evalscope
```


# 运行
```
evalscope perf \
    --model qwen3:1.7b \
    --url 'http://192.168.1.3:11434/v1/chat/completions' \
    --parallel 5 \
    --number 20 \
    --api openai \
    --dataset openqa \
    --stream
```


# 结果
```
'Time taken for tests (s)': 288.8133,
'Number of concurrency': 5,
'Total requests': 10,
'Succeed requests': 10,
'Failed requests': 0,
'Output token throughput (tok/s)': 36.605,
'Total token throughput (tok/s)': 37.5848,
'Request throughput (req/s)': 0.0346,
'Average latency (s)': 112.7879,
'Average time to first token (s)': 22.1013,
'Average time per output token (s)': 0.0865,
'Average input tokens per request': 28.3,
'Average output tokens per request': 1057.2,
'Average package latency (s)': 0.0865,
'Average package per request': 1048.3
```






# reference
- [Best Practices for Evaluating the Qwen3 Model | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- [量化模型效果评估 - Qwen](https://qwen.readthedocs.io/zh-cn/latest/getting_started/quantization_benchmark.html)
- [How Smart is Your AI? Full Assessment of IQ and EQ! | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html)
- 。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</guid><pubDate>Sun, 11 May 2025 15:29:14 +0000</pubDate></item><item><title>MNN-LLM</title><link>https://Ethan-a2.github.io/post/MNN-LLM.html</link><description>

```
git lfs install
git clone https://www.modelscope.cn/qwen/Qwen2-0.5B-Instruct.git

或者
huggingface-cli download taobao-mnn/Qwen2-0.5B-Instruct-MNN --local-dir .//Qwen2-0.5B-Instruct-MNN
```

# export

```
pip3 install torch torchvision torchaudio --force-reinstall
pip install numpy==1.26.4
python llmexport.py --path /media/code/mnn/Qwen2-0.5B-Instruct --export mnn



```



# build
```
cmake -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON -DMNN_BUILD_BENCHMARK=true -DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true ..

make -j16
```



# run
```
./llm_demo /media/code/mnn/MNN/transformers/llm/export/model/config.json
```


# issue
聊天效果很差。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-LLM.html</guid><pubDate>Sat, 10 May 2025 08:01:42 +0000</pubDate></item><item><title>MNN-demo</title><link>https://Ethan-a2.github.io/post/MNN-demo.html</link><description>
# 实例分割
转换时需要添加--keepInputFormat false参数，转为HCHW的格式。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-demo.html</guid><pubDate>Sat, 10 May 2025 05:29:34 +0000</pubDate></item><item><title>MNN multiPose demo</title><link>https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</link><description>
[MNN/demo/exec at master · Ethan-a2/MNN](https://github.com/Ethan-a2/MNN/tree/master/demo/exec)

# build
```
cd path/to/MNN
mkdir build
cd build
cmake -G 'NMake Makefiles' -DCMAKE_BUILD_TYPE=Release -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON ..
make
```


# convert
```
./MNNConvert -f TF --modelFile model-mobilenet_v1_075.pb  --MNNModel model-mobilenet_v1_075.mnn --bizCode biz
```

# excute
```
./multiPose.out model-mobilenet_v1_075.mnn  multipose_input.png  pose.png
```


示例图片在这个路径:
```
../_static/images/start/multipose_input.png
```


参考:
https://mnn-docs.readthedocs.io/en/2.9.6/_sources/start/demo.md.txt

# 报错信息
```
CPU Group: [ 2  0  3  1 ], 800000 - 3500000
The device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0, sme2: 0
        **Tensor shape**: 1, 225, 3, 225, 
Error for compute convolution shape, inputCount:3, outputCount:24, KH:3, KW:3, group:1
inputChannel: 225, batch:1, width:225, height:3. Input data channel may be mismatch with filter channel count
Compute Shape Error for MobilenetV1/add
Invalid Tensor, the session may not be ready
Can't run session because not resized
main, 381, cost time: 0.004000 ms
main, 405, cost time: 0.001000 ms
```


# 解决办法一
将输入的图像转换为tf模块的NHWC的格式。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</guid><pubDate>Sat, 10 May 2025 02:48:11 +0000</pubDate></item><item><title>docker安装浏览器</title><link>https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</link><description>docker-compose.yml:
```
services:
  firefox:
    image: jlesage/firefox
    container_name: firefox
    ports:
      - '5800:5800' # 格式: '宿主机端口:容器端口'
    volumes:
      - /media/docker/firefox/config:/config:rw 
      - /media/docker/firefox/downloads:/downloads:rw
    restart: unless-stopped # 可选：添加重启策略，这样容器意外退出或 Docker 重启时会自动启动
```


# references
- https://github.com/jlesage/docker-firefox
- 。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</guid><pubDate>Sat, 03 May 2025 06:19:22 +0000</pubDate></item><item><title>ipv6内网穿透</title><link>https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</link><description>
# 步骤
1. 光猫改桥接。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</guid><pubDate>Sat, 03 May 2025 03:36:05 +0000</pubDate></item><item><title>国内github加速</title><link>https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</link><description>微软应用商店下载Watt ToolKit
[Watt Toolkit - Free download and install on Windows | Microsoft Store](https://apps.microsoft.com/detail/9mtcfhs560ng?hl=en-US&amp;gl=CN)

![Image](https://github.com/user-attachments/assets/f980382b-eba6-43f4-a747-19fda38544b5)

。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</guid><pubDate>Wed, 30 Apr 2025 21:51:08 +0000</pubDate></item><item><title>github快速搭建博客</title><link>https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</link><description>

# 注意
- 提issue的时候必须加一个label


# issue
- 仓库名为xxx.github.io。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</guid><pubDate>Wed, 30 Apr 2025 21:48:54 +0000</pubDate></item></channel></rss>