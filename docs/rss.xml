<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Brue Leo's Blog</title><link>https://Ethan-a2.github.io</link><description>Freedom is not free.</description><copyright>Brue Leo's Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/61342241?v=4&amp;size=64</url><title>avatar</title><link>https://Ethan-a2.github.io</link></image><lastBuildDate>Sun, 18 May 2025 04:22:09 +0000</lastBuildDate><managingEditor>Brue Leo's Blog</managingEditor><ttl>60</ttl><webMaster>Brue Leo's Blog</webMaster><item><title>minimind.inference</title><link>https://Ethan-a2.github.io/post/minimind.inference.html</link><description>
# download
```
git clone https://github.com/jingyaogong/minimind.git
huggingface-cli download --local-dir MiniMind2
```

# test
```
python eval_model.py --load 1 --model_mode 2

cd scripts
streamlit run web_demo.py

```

# ä½¿ç”¨llama.cppæ¨ç†
åœ¨llama.cppä¸­æ‰“ä¸Šå¦‚ä¸‹patch:
```
diff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py
index bf6bc683..5ea853b6 100755
--- a/convert_hf_to_gguf.py
+++ b/convert_hf_to_gguf.py
@@ -808,7 +808,8 @@ class TextModel(ModelBase):
             logger.warning(f'** chkhsh:  {chkhsh}')
             logger.warning('**************************************************************************************')
             logger.warning('\n')
-            raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
+            res = 'smollm'
+            #raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
 
         logger.debug(f'tokenizer.ggml.pre: {repr(res)}')
         logger.debug(f'chkhsh: {chkhsh}')
```

```
python convert_hf_to_gguf.py ../minimind/MiniMind2/

./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M

./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml

```

# ä½¿ç”¨ollamaéƒ¨ç½²
æ–°å»ºminimind.modelfileï¼š
```
FROM ./MiniMind2-109M-F16.gguf
TEMPLATE '''{{ if .System }}&lt;|im_start|&gt;system
{{ .System }}&lt;|im_end|&gt;
{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user
{{ .Prompt }}&lt;|im_end|&gt;
{{ end }}&lt;|im_start|&gt;assistant
'''
```

åŠ è½½æ¨¡å‹å¹¶å‘½åä¸ºminimind2
```
ollama create -f minimind.modelfile minimind2
```

å¯åŠ¨æ¨ç†

```
ollama run minimind2
```

# reference
[jingyaogong/minimind: ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!](https://github.com/jingyaogong/minimind#vllm%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1)
ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/minimind.inference.html</guid><pubDate>Sun, 18 May 2025 04:21:37 +0000</pubDate></item><item><title>llama3.cppåŸºäºNVIDIA1060è¿›è¡Œå›¾ç‰‡å¯¹è¯</title><link>https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</link><description>
# ç¯å¢ƒ
æ˜¾å¡ï¼šNVIDIA GeForce GTX 1060
å›¾ç‰‡å¯¹è¯æ¨¡å‹ï¼šSmolVLM-500M-Instruct



# ç¼–è¯‘

```
sudo apt-get install libssl-dev libcurl4-openssl-dev

git clone https://github.com/ggml-org/llama.cpp.git
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=61
make -j16
```


åœ¨è¿™é‡Œ[Ollama GPUæ”¯æŒ - Nvidiaå’ŒAMD GPUå…¼å®¹æ€§ | LlamaFactory | LlamaFactory](https://www.llamafactory.cn/ollama-docs/gpu.html#google_vignette)æ‰¾åˆ°1060çš„æ¶æ„æ˜¯6.1ï¼Œç¼–è¯‘æ—¶æŒ‡å®š-DCMAKE_CUDA_ARCHITECTURES=61


# æ¨¡å‹GGUFä¸‹è½½
```
export HF_HOME=/media/dataset/hf
export HF_ENDPOINT=https://hf-mirror.com
export HUGGINGFACE_TOKEN=xxx

huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF
huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF --local-dir .
```


# è¿è¡Œllama3.cpp
```
export PATH'$PATH:/path/to/llama.cpp/build/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/llama.cpp/build/bin

llama-server -m ./SmolVLM-500M-Instruct-Q8_0.gguf --mmproj mmproj-SmolVLM-500M-Instruct-Q8_0.gguf --host 0.0.0.0 --port 8080 -ngl 100
```


è¾“å‡ºæ—¥å¿—:
```
User: Hello&lt;end_of_utterance&gt;
Assistant: Hi there&lt;end_of_utterance&gt;
User: How are you?&lt;end_of_utterance&gt;
Assistant:'
main: server is listening on http://0.0.0.0:8080 - starting the main loop
srv  update_slots: all slots are idle
```


- ç”¨æµè§ˆå™¨æ‰“å¼€ç½‘å€ï¼Œæ¯”å¦‚ 192.168.3.100:1080ï¼Œå³å¯ä»¥è¿›è¡Œå›¾ç‰‡å¯¹è¯ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</guid><pubDate>Wed, 14 May 2025 15:00:48 +0000</pubDate></item><item><title>Qwen3æ€§èƒ½è¯„æµ‹-evalscope</title><link>https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</link><description>



# ç¯å¢ƒå‡†å¤‡ 
```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1060        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   62C    P8               3W /  78W |   2426MiB /  6144MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                            4MiB |
|    0   N/A  N/A      3029      C   /usr/local/bin/ollama                      2418MiB |
+---------------------------------------------------------------------------------------+
```


```
ollama run qwen3:1.7b
pip install evalscope
```


# è¿è¡Œ
```
evalscope perf \
    --model qwen3:1.7b \
    --url 'http://192.168.1.3:11434/v1/chat/completions' \
    --parallel 5 \
    --number 20 \
    --api openai \
    --dataset openqa \
    --stream
```


# ç»“æœ
```
'Time taken for tests (s)': 288.8133,
'Number of concurrency': 5,
'Total requests': 10,
'Succeed requests': 10,
'Failed requests': 0,
'Output token throughput (tok/s)': 36.605,
'Total token throughput (tok/s)': 37.5848,
'Request throughput (req/s)': 0.0346,
'Average latency (s)': 112.7879,
'Average time to first token (s)': 22.1013,
'Average time per output token (s)': 0.0865,
'Average input tokens per request': 28.3,
'Average output tokens per request': 1057.2,
'Average package latency (s)': 0.0865,
'Average package per request': 1048.3
```






# reference
- [Best Practices for Evaluating the Qwen3 Model | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- [é‡åŒ–æ¨¡å‹æ•ˆæœè¯„ä¼° - Qwen](https://qwen.readthedocs.io/zh-cn/latest/getting_started/quantization_benchmark.html)
- [How Smart is Your AI? Full Assessment of IQ and EQ! | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html)
- ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</guid><pubDate>Sun, 11 May 2025 15:29:14 +0000</pubDate></item><item><title>MNN-LLM</title><link>https://Ethan-a2.github.io/post/MNN-LLM.html</link><description>

```
git lfs install
git clone https://www.modelscope.cn/qwen/Qwen2-0.5B-Instruct.git

æˆ–è€…
huggingface-cli download taobao-mnn/Qwen2-0.5B-Instruct-MNN --local-dir .//Qwen2-0.5B-Instruct-MNN
```

# export

```
pip3 install torch torchvision torchaudio --force-reinstall
pip install numpy==1.26.4
python llmexport.py --path /media/code/mnn/Qwen2-0.5B-Instruct --export mnn



```



# build
```
cmake -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON -DMNN_BUILD_BENCHMARK=true -DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true ..

make -j16
```



# run
```
./llm_demo /media/code/mnn/MNN/transformers/llm/export/model/config.json
```


# issue
èŠå¤©æ•ˆæœå¾ˆå·®ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-LLM.html</guid><pubDate>Sat, 10 May 2025 08:01:42 +0000</pubDate></item><item><title>MNN-demo</title><link>https://Ethan-a2.github.io/post/MNN-demo.html</link><description>
# å®ä¾‹åˆ†å‰²
è½¬æ¢æ—¶éœ€è¦æ·»åŠ --keepInputFormat falseå‚æ•°ï¼Œè½¬ä¸ºHCHWçš„æ ¼å¼ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-demo.html</guid><pubDate>Sat, 10 May 2025 05:29:34 +0000</pubDate></item><item><title>MNN multiPose demo</title><link>https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</link><description>
[MNN/demo/exec at master Â· Ethan-a2/MNN](https://github.com/Ethan-a2/MNN/tree/master/demo/exec)

# build
```
cd path/to/MNN
mkdir build
cd build
cmake -G 'NMake Makefiles' -DCMAKE_BUILD_TYPE=Release -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON ..
make
```


# convert
```
./MNNConvert -f TF --modelFile model-mobilenet_v1_075.pb  --MNNModel model-mobilenet_v1_075.mnn --bizCode biz
```

# excute
```
./multiPose.out model-mobilenet_v1_075.mnn  multipose_input.png  pose.png
```


ç¤ºä¾‹å›¾ç‰‡åœ¨è¿™ä¸ªè·¯å¾„:
```
../_static/images/start/multipose_input.png
```


å‚è€ƒ:
https://mnn-docs.readthedocs.io/en/2.9.6/_sources/start/demo.md.txt

# æŠ¥é”™ä¿¡æ¯
```
CPU Group: [ 2  0  3  1 ], 800000 - 3500000
The device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0, sme2: 0
        **Tensor shape**: 1, 225, 3, 225, 
Error for compute convolution shape, inputCount:3, outputCount:24, KH:3, KW:3, group:1
inputChannel: 225, batch:1, width:225, height:3. Input data channel may be mismatch with filter channel count
Compute Shape Error for MobilenetV1/add
Invalid Tensor, the session may not be ready
Can't run session because not resized
main, 381, cost time: 0.004000 ms
main, 405, cost time: 0.001000 ms
```


# è§£å†³åŠæ³•ä¸€
å°†è¾“å…¥çš„å›¾åƒè½¬æ¢ä¸ºtfæ¨¡å—çš„NHWCçš„æ ¼å¼ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</guid><pubDate>Sat, 10 May 2025 02:48:11 +0000</pubDate></item><item><title>dockerå®‰è£…æµè§ˆå™¨</title><link>https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</link><description>docker-compose.yml:
```
services:
  firefox:
    image: jlesage/firefox
    container_name: firefox
    ports:
      - '5800:5800' # æ ¼å¼: 'å®¿ä¸»æœºç«¯å£:å®¹å™¨ç«¯å£'
    volumes:
      - /media/docker/firefox/config:/config:rw 
      - /media/docker/firefox/downloads:/downloads:rw
    restart: unless-stopped # å¯é€‰ï¼šæ·»åŠ é‡å¯ç­–ç•¥ï¼Œè¿™æ ·å®¹å™¨æ„å¤–é€€å‡ºæˆ– Docker é‡å¯æ—¶ä¼šè‡ªåŠ¨å¯åŠ¨
```


# references
- https://github.com/jlesage/docker-firefox
- ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</guid><pubDate>Sat, 03 May 2025 06:19:22 +0000</pubDate></item><item><title>ipv6å†…ç½‘ç©¿é€</title><link>https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</link><description>
# æ­¥éª¤
1. å…‰çŒ«æ”¹æ¡¥æ¥ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</guid><pubDate>Sat, 03 May 2025 03:36:05 +0000</pubDate></item><item><title>å›½å†…githubåŠ é€Ÿ</title><link>https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</link><description>å¾®è½¯åº”ç”¨å•†åº—ä¸‹è½½Watt ToolKit
[Watt Toolkit - Free download and install on Windows | Microsoft Store](https://apps.microsoft.com/detail/9mtcfhs560ng?hl=en-US&amp;gl=CN)

![Image](https://github.com/user-attachments/assets/f980382b-eba6-43f4-a747-19fda38544b5)

ã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</guid><pubDate>Wed, 30 Apr 2025 21:51:08 +0000</pubDate></item><item><title>githubå¿«é€Ÿæ­å»ºåšå®¢</title><link>https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</link><description>

# æ³¨æ„
- æissueçš„æ—¶å€™å¿…é¡»åŠ ä¸€ä¸ªlabel


# issue
- ä»“åº“åä¸ºxxx.github.ioã€‚</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</guid><pubDate>Wed, 30 Apr 2025 21:48:54 +0000</pubDate></item></channel></rss>