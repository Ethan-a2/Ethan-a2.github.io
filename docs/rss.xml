<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Brue Leo's Blog</title><link>https://Ethan-a2.github.io</link><description>Freedom is not free.</description><copyright>Brue Leo's Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/61342241?v=4&amp;size=64</url><title>avatar</title><link>https://Ethan-a2.github.io</link></image><lastBuildDate>Sun, 03 Aug 2025 11:07:26 +0000</lastBuildDate><managingEditor>Brue Leo's Blog</managingEditor><ttl>60</ttl><webMaster>Brue Leo's Blog</webMaster><item><title>小米15 NPU部署Llama-3.2-1B-Instruct</title><link>https://Ethan-a2.github.io/post/xiao-mi-15%20NPU-bu-shu-Llama-3.2-1B-Instruct.html</link><description># 准备
- https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
- 显存至少需要12G。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/xiao-mi-15%20NPU-bu-shu-Llama-3.2-1B-Instruct.html</guid><pubDate>Sun, 03 Aug 2025 11:06:50 +0000</pubDate></item><item><title>HexagonNPU运行llama.cpp</title><link>https://Ethan-a2.github.io/post/HexagonNPU-yun-xing-llama.cpp.html</link><description>本文将详细介y绍如何在 Android 设备上，利用 `llama.cpp` 框架，结合高通神经网络处理单元 (QNN) 的硬件加速能力，部署并运行 Qwen 1.5 模型。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/HexagonNPU-yun-xing-llama.cpp.html</guid><pubDate>Tue, 22 Jul 2025 13:01:57 +0000</pubDate></item><item><title>Redmi K70 Pro运行Llama 1.1B</title><link>https://Ethan-a2.github.io/post/Redmi%20K70%20Pro-yun-xing-Llama%201.1B.html</link><description>
# 前言

随着移动设备硬件性能的飞速发展，将大型语言模型（LLM）部署到端侧设备上运行，实现低延迟、保护用户隐私的AI推理，正成为一个热门方向。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Redmi%20K70%20Pro-yun-xing-Llama%201.1B.html</guid><pubDate>Thu, 17 Jul 2025 15:31:10 +0000</pubDate></item><item><title>小米15Ultra运行Qwen2-7B-Instruct</title><link>https://Ethan-a2.github.io/post/xiao-mi-15Ultra-yun-xing-Qwen2-7B-Instruct.html</link><description>
# environment
- Operating System: Ubuntu 22.04
- QNN SDK Version: 2.36.0.250627
- devices：xiaomi 15Ultra 16G内存, Snapdragon® 8 Elite Mobile
- genie-t2t-run

- 注意，[Qwen2-7B-Instruct - Qualcomm AI Hub](https://aihub.qualcomm.com/models/qwen2_7b_instruct)目前只支持Snapdragon® 8 Elite Mobile。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/xiao-mi-15Ultra-yun-xing-Qwen2-7B-Instruct.html</guid><pubDate>Tue, 15 Jul 2025 14:03:37 +0000</pubDate></item><item><title>0713.Xiaomi14Ultra通过QNN运行Qwen</title><link>https://Ethan-a2.github.io/post/0713.Xiaomi14Ultra-tong-guo-QNN-yun-xing-Qwen.html</link><description>
# environment
- Operating System: Ubuntu 22.04
- QNN SDK Version: 2.31.0.250130
- Hexagon SDK Version: 5.5.5.0
- Android NDK Version: android-ndk-r26d
- devices：xiaomi 14Ultra, 8gen3
- mllm



# build
```
cd /opt/qcom/Hexagon_SDK
ln -s 5.5.5.0 HexagonSDK

export QNN_SDK_ROOT=/opt/qcom/aistack/qairt/2.31.0.250130
source $QNN_SDK_ROOT/bin/envsetup.sh
source /opt/qcom/Hexagon_SDK/5.5.5.0/setup_sdk_env.source

git clone https://github.com/UbiquitousLearning/mllm.git

CMakeLists.txt 中gtest相关的注释掉



cd src/backends/qnn
ln -s $QNN_SDK_ROOT sdk

cd src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/
make htp_aarch64 &amp;&amp; make htp_v75

./build_android_qnn.sh
./run_qwen_qnn.sh

要加下这两句:
adb shell mkdir -p /data/local/tmp/mllm/bin
adb push ../vocab/qwen_merges.txt /data/local/tmp/mllm/vocab/
```


# result
```
./run_qwen_qnn.sh 
../vocab/qwen_vocab.mllm: 1 file pushed, 0 skipped. 95.0 MB/s (3189278 bytes in 0.032s)
../vocab/qwen_merges.txt: 1 file pushed, 0 skipped. 81.8 MB/s (1671839 bytes in 0.019s)
qwen-1.5-1.8b-chat-int8 file already exists
qwen-1.5-1.8b-chat-q4k.mllm file already exists
QNN_SDK_ROOT is set to /opt/qcom/aistack/qairt/2.31.0.250130
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtp.so: 1 file pushed, 0 skipped. 53.6 MB/s (2060792 bytes in 0.037s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpV75Stub.so: 1 file pushed, 0 skipped. 22.9 MB/s (453640 bytes in 0.019s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpPrepare.so: 1 file pushed, 0 skipped. 33.4 MB/s (55080032 bytes in 1.575s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpProfilingReader.so: 1 file pushed, 0 skipped. 113.0 MB/s (579072 bytes in 0.005s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpOptraceProfilingReader.so: 1 file pushed, 0 skipped. 153.1 MB/s (3073968 bytes in 0.019s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpV75CalculatorStub.so: 1 file pushed, 0 skipped. 0.4 MB/s (6472 bytes in 0.015s)
/opt/qcom/aistack/qairt/2.31.0.250130/lib/hexagon-v75/unsigned/libQnnHtpV75Skel.so: 1 file pushed, 0 skipped. 89.1 MB/s (8485728 bytes in 0.091s)
../src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/build/aarch64-android/libQnnLLaMAPackage.so: 1 file pushed, 0 skipped. 300.3 MB/s (1479704 bytes in 0.005s)
../src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/build/hexagon-v75/libQnnLLaMAPackage.so: 1 file pushed, 0 skipped. 177.0 MB/s (818800 bytes in 0.004s)
../bin-arm/demo_qwen_npu: 1 file pushed, 0 skipped. 72.1 MB/s (93400608 bytes in 1.235s)
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:118] Backend: libQnnHtp.so
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:143] Backend        build version: v2.31.0.250130151446_114721
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:166] Initialize Backend Returned Status = 0
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:636] Registered Op Package: libQnnLLaMAPackage_CPU.so and interface provider: LLaMAPackageInterfaceProvider
[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:636] Registered Op Package: libQnnLLaMAPackage_HTP.so and interface provider: LLaMAPackageInterfaceProvider
Warmup finished.
[Q] 'Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. These models are trained on vast amounts of data, enabling them to perform a wide range of tasks, from answering questions and summarizing text to generating creative content and engaging in conversational dialogue. LLMs like GPT-3 and GPT-4, developed by OpenAI, have set new benchmarks in natural language processing by leveraging deep learning architectures, particularly transformer models, which excel at capturing context and relationships within text. The scalability and versatility of LLMs make them invaluable tools for applications in education, customer service, content creation, and more. However, their deployment also raises ethical considerations, including issues of bias, misinformation, and the potential for misuse. As the field continues to evolve, ongoing research and responsible deployment strategies are essential to harnessing the full potential of these powerful AI systems while mitigating their risks.'
Generate a title based on the above text.
[A] Exploring the Advantages and Ethical Considerations of Large Language Models (LLMs): A Comprehensive OverviewSegmentation fault 
```


# issue
- 速度很慢，不了解原因


# references
- [mllm/src/backends/qnn/README.md at main · UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm/blob/main/src/backends/qnn/README.md)
- 
。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/0713.Xiaomi14Ultra-tong-guo-QNN-yun-xing-Qwen.html</guid><pubDate>Sun, 13 Jul 2025 07:04:58 +0000</pubDate></item><item><title>Android K70 Pro llama.cpp Gemma-3n</title><link>https://Ethan-a2.github.io/post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html</link><description># 设备
- K70 Pro Adreno 750 (Snapdragon 8 Gen 3)
- NDK26
- Adreno使用opencl编译


# 模型
- gemma-3n-E2B-it-UD-Q4_K_XL.gguf


# Android CPU上运行
```
ndk-r26d:
cmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-28 -DCMAKE_C_FLAGS='-march=armv8.7a' -DCMAKE_CXX_FLAGS='-march=armv8.7a' -DGGML_OPENMP=OFF  -DGGML_LLAMAFILE=OFF -DLLAMA_CURL=OFF -B build-android
cmake --build build -j16
cmake --install build-android --prefix ../install

cd ../install
adb shell 'mkdir /data/local/tmp/llama.cpp'
adb push install/. /data/local/tmp/llama.cpp/
adb push {model}.gguf /data/local/tmp/llama.cpp/
adb shell 'ls /data/local/tmp/llama.cpp'
adb shell
cd /data/local/tmp/llama.cpp/lib
export LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/lib:$LD_LIBRARY_PATH
./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose
```


# Adreno上运行
```
Adreno:

Install OpenCL Headers and Library
git clone https://github.com/KhronosGroup/OpenCL-Headers &amp;&amp; \
cd OpenCL-Headers &amp;&amp; \
cp -rf ./CL/ $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include

git clone https://github.com/KhronosGroup/OpenCL-ICD-Loader &amp;&amp; \
cd OpenCL-ICD-Loader &amp;&amp; \
mkdir build_ndk26 &amp;&amp; cd build_ndk26
cmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=24 -DANDROID_STL=c++_shared
cp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android

Build llama.cpp:
cmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a  -DANDROID_PLATFORM=android-28  -DBUILD_SHARED_LIBS=OFF -DGGML_OPENCL=ON  -DLLAMA_CURL=OFF
ninja -j16
DESTDIR=../install-opencl/ ninja install


cd ../install-opencl/usr/loca/
adb shell 'mkdir /data/local/tmp/llama.cpp-opencl'
adb push . /data/local/tmp/llama.cpp-opencl/
adb push {model}.gguf /data/local/tmp/llama.cpp-opencl/
adb shell 'ls /data/local/tmp/llama.cpp-opencl'
adb shell
cd /data/local/tmp/llama.cpp-opencl/lib
export LD_LIBRARY_PATH=/data/local/tmp/llama.cpp-opencl/lib:$LD_LIBRARY_PATH
./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose
```


# Reference
- https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md
- https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md
- [google-ai-edge](https://github.com/google-ai-edge)

# issue
- 用CPU基本与Google官方的gallery性能一致。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html</guid><pubDate>Sun, 29 Jun 2025 09:16:17 +0000</pubDate></item><item><title>minimind.inference</title><link>https://Ethan-a2.github.io/post/minimind.inference.html</link><description>
# download
```
git clone https://github.com/jingyaogong/minimind.git
huggingface-cli download --local-dir MiniMind2
```

# test
```
python eval_model.py --load 1 --model_mode 2

cd scripts
streamlit run web_demo.py

```

# 使用llama.cpp推理
在llama.cpp中打上如下patch:
```
diff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py
index bf6bc683..5ea853b6 100755
--- a/convert_hf_to_gguf.py
+++ b/convert_hf_to_gguf.py
@@ -808,7 +808,8 @@ class TextModel(ModelBase):
             logger.warning(f'** chkhsh:  {chkhsh}')
             logger.warning('**************************************************************************************')
             logger.warning('\n')
-            raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
+            res = 'smollm'
+            #raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')
 
         logger.debug(f'tokenizer.ggml.pre: {repr(res)}')
         logger.debug(f'chkhsh: {chkhsh}')
```

```
python convert_hf_to_gguf.py ../minimind/MiniMind2/

./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M

./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml

```

# 使用ollama部署
新建minimind.modelfile：
```
FROM ./MiniMind2-109M-F16.gguf
TEMPLATE '''{{ if .System }}&lt;|im_start|&gt;system
{{ .System }}&lt;|im_end|&gt;
{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user
{{ .Prompt }}&lt;|im_end|&gt;
{{ end }}&lt;|im_start|&gt;assistant
'''
```

加载模型并命名为minimind2
```
ollama create -f minimind.modelfile minimind2
```

启动推理

```
ollama run minimind2
```

# reference
[jingyaogong/minimind: 🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!](https://github.com/jingyaogong/minimind#vllm%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1)
。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/minimind.inference.html</guid><pubDate>Sun, 18 May 2025 04:21:37 +0000</pubDate></item><item><title>llama3.cpp基于NVIDIA1060进行图片对话</title><link>https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</link><description>
# 环境
显卡：NVIDIA GeForce GTX 1060
图片对话模型：SmolVLM-500M-Instruct



# 编译

```
sudo apt-get install libssl-dev libcurl4-openssl-dev

git clone https://github.com/ggml-org/llama.cpp.git
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=61
make -j16
```


在这里[Ollama GPU支持 - Nvidia和AMD GPU兼容性 | LlamaFactory | LlamaFactory](https://www.llamafactory.cn/ollama-docs/gpu.html#google_vignette)找到1060的架构是6.1，编译时指定-DCMAKE_CUDA_ARCHITECTURES=61


# 模型GGUF下载
```
export HF_HOME=/media/dataset/hf
export HF_ENDPOINT=https://hf-mirror.com
export HUGGINGFACE_TOKEN=xxx

huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF
huggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF --local-dir .
```


# 运行llama3.cpp
```
export PATH'$PATH:/path/to/llama.cpp/build/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/llama.cpp/build/bin

llama-server -m ./SmolVLM-500M-Instruct-Q8_0.gguf --mmproj mmproj-SmolVLM-500M-Instruct-Q8_0.gguf --host 0.0.0.0 --port 8080 -ngl 100
```


输出日志:
```
User: Hello&lt;end_of_utterance&gt;
Assistant: Hi there&lt;end_of_utterance&gt;
User: How are you?&lt;end_of_utterance&gt;
Assistant:'
main: server is listening on http://0.0.0.0:8080 - starting the main loop
srv  update_slots: all slots are idle
```


- 用浏览器打开网址，比如 192.168.3.100:1080，即可以进行图片对话。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html</guid><pubDate>Wed, 14 May 2025 15:00:48 +0000</pubDate></item><item><title>Qwen3性能评测-evalscope</title><link>https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</link><description>



# 环境准备 
```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1060        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   62C    P8               3W /  78W |   2426MiB /  6144MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                            4MiB |
|    0   N/A  N/A      3029      C   /usr/local/bin/ollama                      2418MiB |
+---------------------------------------------------------------------------------------+
```


```
ollama run qwen3:1.7b
pip install evalscope
```


# 运行
```
evalscope perf \
    --model qwen3:1.7b \
    --url 'http://192.168.1.3:11434/v1/chat/completions' \
    --parallel 5 \
    --number 20 \
    --api openai \
    --dataset openqa \
    --stream
```


# 结果
```
'Time taken for tests (s)': 288.8133,
'Number of concurrency': 5,
'Total requests': 10,
'Succeed requests': 10,
'Failed requests': 0,
'Output token throughput (tok/s)': 36.605,
'Total token throughput (tok/s)': 37.5848,
'Request throughput (req/s)': 0.0346,
'Average latency (s)': 112.7879,
'Average time to first token (s)': 22.1013,
'Average time per output token (s)': 0.0865,
'Average input tokens per request': 28.3,
'Average output tokens per request': 1057.2,
'Average package latency (s)': 0.0865,
'Average package per request': 1048.3
```






# reference
- [Best Practices for Evaluating the Qwen3 Model | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- [量化模型效果评估 - Qwen](https://qwen.readthedocs.io/zh-cn/latest/getting_started/quantization_benchmark.html)
- [How Smart is Your AI? Full Assessment of IQ and EQ! | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html)
- 。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/Qwen3-xing-neng-ping-ce--evalscope.html</guid><pubDate>Sun, 11 May 2025 15:29:14 +0000</pubDate></item><item><title>MNN-LLM</title><link>https://Ethan-a2.github.io/post/MNN-LLM.html</link><description>

```
git lfs install
git clone https://www.modelscope.cn/qwen/Qwen2-0.5B-Instruct.git

或者
huggingface-cli download taobao-mnn/Qwen2-0.5B-Instruct-MNN --local-dir .//Qwen2-0.5B-Instruct-MNN
```

# export

```
pip3 install torch torchvision torchaudio --force-reinstall
pip install numpy==1.26.4
python llmexport.py --path /media/code/mnn/Qwen2-0.5B-Instruct --export mnn



```



# build
```
cmake -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON -DMNN_BUILD_BENCHMARK=true -DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true ..

make -j16
```



# run
```
./llm_demo /media/code/mnn/MNN/transformers/llm/export/model/config.json
```


# issue
聊天效果很差。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-LLM.html</guid><pubDate>Sat, 10 May 2025 08:01:42 +0000</pubDate></item><item><title>MNN-demo</title><link>https://Ethan-a2.github.io/post/MNN-demo.html</link><description>
# 实例分割
转换时需要添加--keepInputFormat false参数，转为HCHW的格式。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN-demo.html</guid><pubDate>Sat, 10 May 2025 05:29:34 +0000</pubDate></item><item><title>MNN multiPose demo</title><link>https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</link><description>
[MNN/demo/exec at master · Ethan-a2/MNN](https://github.com/Ethan-a2/MNN/tree/master/demo/exec)

# build
```
cd path/to/MNN
mkdir build
cd build
cmake -G 'NMake Makefiles' -DCMAKE_BUILD_TYPE=Release -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON ..
make
```


# convert
```
./MNNConvert -f TF --modelFile model-mobilenet_v1_075.pb  --MNNModel model-mobilenet_v1_075.mnn --bizCode biz
```

# excute
```
./multiPose.out model-mobilenet_v1_075.mnn  multipose_input.png  pose.png
```


示例图片在这个路径:
```
../_static/images/start/multipose_input.png
```


参考:
https://mnn-docs.readthedocs.io/en/2.9.6/_sources/start/demo.md.txt

# 报错信息
```
CPU Group: [ 2  0  3  1 ], 800000 - 3500000
The device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0, sme2: 0
        **Tensor shape**: 1, 225, 3, 225, 
Error for compute convolution shape, inputCount:3, outputCount:24, KH:3, KW:3, group:1
inputChannel: 225, batch:1, width:225, height:3. Input data channel may be mismatch with filter channel count
Compute Shape Error for MobilenetV1/add
Invalid Tensor, the session may not be ready
Can't run session because not resized
main, 381, cost time: 0.004000 ms
main, 405, cost time: 0.001000 ms
```


# 解决办法一
将输入的图像转换为tf模块的NHWC的格式。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/MNN%20multiPose%20demo.html</guid><pubDate>Sat, 10 May 2025 02:48:11 +0000</pubDate></item><item><title>docker安装浏览器</title><link>https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</link><description>docker-compose.yml:
```
services:
  firefox:
    image: jlesage/firefox
    container_name: firefox
    ports:
      - '5800:5800' # 格式: '宿主机端口:容器端口'
    volumes:
      - /media/docker/firefox/config:/config:rw 
      - /media/docker/firefox/downloads:/downloads:rw
    restart: unless-stopped # 可选：添加重启策略，这样容器意外退出或 Docker 重启时会自动启动
```


# references
- https://github.com/jlesage/docker-firefox
- 。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/docker-an-zhuang-liu-lan-qi.html</guid><pubDate>Sat, 03 May 2025 06:19:22 +0000</pubDate></item><item><title>ipv6内网穿透</title><link>https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</link><description>
# 步骤
1. 光猫改桥接。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/ipv6-nei-wang-chuan-tou.html</guid><pubDate>Sat, 03 May 2025 03:36:05 +0000</pubDate></item><item><title>国内github加速</title><link>https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</link><description>微软应用商店下载Watt ToolKit
[Watt Toolkit - Free download and install on Windows | Microsoft Store](https://apps.microsoft.com/detail/9mtcfhs560ng?hl=en-US&amp;gl=CN)

![Image](https://github.com/user-attachments/assets/f980382b-eba6-43f4-a747-19fda38544b5)

。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/guo-nei-github-jia-su.html</guid><pubDate>Wed, 30 Apr 2025 21:51:08 +0000</pubDate></item><item><title>github快速搭建博客</title><link>https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</link><description>

# 注意
- 提issue的时候必须加一个label


# issue
- 仓库名为xxx.github.io。</description><guid isPermaLink="true">https://Ethan-a2.github.io/post/github-kuai-su-da-jian-bo-ke.html</guid><pubDate>Wed, 30 Apr 2025 21:48:54 +0000</pubDate></item></channel></rss>