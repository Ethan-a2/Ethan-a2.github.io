{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "Brue Leo's Blog", "subTitle": "Freedom is not free.", "avatarUrl": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "GMEEK_VERSION": "last", "postListJson": {"P1": {"htmlDir": "docs/post/github-kuai-su-da-jian-bo-ke.html", "labels": ["documentation"], "postTitle": "github\u5feb\u901f\u642d\u5efa\u535a\u5ba2", "postUrl": "post/github-kuai-su-da-jian-bo-ke.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/1", "commentNum": 0, "wordCount": 439, "description": "\n\n# \u6ce8\u610f\n- \u63d0issue\u7684\u65f6\u5019\u5fc5\u987b\u52a0\u4e00\u4e2alabel\n\n\n# issue\n- \u4ed3\u5e93\u540d\u4e3axxx.github.io\u3002", "top": 0, "createdAt": 1746049734, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-01", "dateLabelColor": "#0969da"}, "P2": {"htmlDir": "docs/post/guo-nei-github-jia-su.html", "labels": ["documentation"], "postTitle": "\u56fd\u5185github\u52a0\u901f", "postUrl": "post/guo-nei-github-jia-su.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/2", "commentNum": 0, "wordCount": 248, "description": "\u5fae\u8f6f\u5e94\u7528\u5546\u5e97\u4e0b\u8f7dWatt ToolKit\n[Watt Toolkit - Free download and install on Windows | Microsoft Store](https://apps.microsoft.com/detail/9mtcfhs560ng?hl=en-US&gl=CN)\n\n![Image](https://github.com/user-attachments/assets/f980382b-eba6-43f4-a747-19fda38544b5)\n\n\u3002", "top": 0, "createdAt": 1746049868, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-01", "dateLabelColor": "#0969da"}, "P3": {"htmlDir": "docs/post/ipv6-nei-wang-chuan-tou.html", "labels": ["documentation"], "postTitle": "ipv6\u5185\u7f51\u7a7f\u900f", "postUrl": "post/ipv6-nei-wang-chuan-tou.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/3", "commentNum": 0, "wordCount": 8728, "description": "\n# \u6b65\u9aa4\n1. \u5149\u732b\u6539\u6865\u63a5\u3002", "top": 0, "createdAt": 1746243365, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-03", "dateLabelColor": "#0969da"}, "P4": {"htmlDir": "docs/post/docker-an-zhuang-liu-lan-qi.html", "labels": ["documentation"], "postTitle": "docker\u5b89\u88c5\u6d4f\u89c8\u5668", "postUrl": "post/docker-an-zhuang-liu-lan-qi.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/4", "commentNum": 0, "wordCount": 397, "description": "docker-compose.yml:\n```\nservices:\n  firefox:\n    image: jlesage/firefox\n    container_name: firefox\n    ports:\n      - '5800:5800' # \u683c\u5f0f: '\u5bbf\u4e3b\u673a\u7aef\u53e3:\u5bb9\u5668\u7aef\u53e3'\n    volumes:\n      - /media/docker/firefox/config:/config:rw \n      - /media/docker/firefox/downloads:/downloads:rw\n    restart: unless-stopped # \u53ef\u9009\uff1a\u6dfb\u52a0\u91cd\u542f\u7b56\u7565\uff0c\u8fd9\u6837\u5bb9\u5668\u610f\u5916\u9000\u51fa\u6216 Docker \u91cd\u542f\u65f6\u4f1a\u81ea\u52a8\u542f\u52a8\n```\n\n\n# references\n- https://github.com/jlesage/docker-firefox\n- \u3002", "top": 0, "createdAt": 1746253162, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-03", "dateLabelColor": "#0969da"}, "P5": {"htmlDir": "docs/post/MNN multiPose demo.html", "labels": ["MNN"], "postTitle": "MNN multiPose demo", "postUrl": "post/MNN%20multiPose%20demo.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/5", "commentNum": 0, "wordCount": 2413, "description": "\n[MNN/demo/exec at master \u00b7 Ethan-a2/MNN](https://github.com/Ethan-a2/MNN/tree/master/demo/exec)\n\n# build\n```\ncd path/to/MNN\nmkdir build\ncd build\ncmake -G 'NMake Makefiles' -DCMAKE_BUILD_TYPE=Release -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON ..\nmake\n```\n\n\n# convert\n```\n./MNNConvert -f TF --modelFile model-mobilenet_v1_075.pb  --MNNModel model-mobilenet_v1_075.mnn --bizCode biz\n```\n\n# excute\n```\n./multiPose.out model-mobilenet_v1_075.mnn  multipose_input.png  pose.png\n```\n\n\n\u793a\u4f8b\u56fe\u7247\u5728\u8fd9\u4e2a\u8def\u5f84:\n```\n../_static/images/start/multipose_input.png\n```\n\n\n\u53c2\u8003:\nhttps://mnn-docs.readthedocs.io/en/2.9.6/_sources/start/demo.md.txt\n\n# \u62a5\u9519\u4fe1\u606f\n```\nCPU Group: [ 2  0  3  1 ], 800000 - 3500000\nThe device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0, sme2: 0\n        **Tensor shape**: 1, 225, 3, 225, \nError for compute convolution shape, inputCount:3, outputCount:24, KH:3, KW:3, group:1\ninputChannel: 225, batch:1, width:225, height:3. Input data channel may be mismatch with filter channel count\nCompute Shape Error for MobilenetV1/add\nInvalid Tensor, the session may not be ready\nCan't run session because not resized\nmain, 381, cost time: 0.004000 ms\nmain, 405, cost time: 0.001000 ms\n```\n\n\n# \u89e3\u51b3\u529e\u6cd5\u4e00\n\u5c06\u8f93\u5165\u7684\u56fe\u50cf\u8f6c\u6362\u4e3atf\u6a21\u5757\u7684NHWC\u7684\u683c\u5f0f\u3002", "top": 0, "createdAt": 1746845291, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-10", "dateLabelColor": "#0969da"}, "P6": {"htmlDir": "docs/post/MNN-demo.html", "labels": ["MNN"], "postTitle": "MNN-demo", "postUrl": "post/MNN-demo.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/6", "commentNum": 0, "wordCount": 2302, "description": "\n# \u5b9e\u4f8b\u5206\u5272\n\u8f6c\u6362\u65f6\u9700\u8981\u6dfb\u52a0--keepInputFormat false\u53c2\u6570\uff0c\u8f6c\u4e3aHCHW\u7684\u683c\u5f0f\u3002", "top": 0, "createdAt": 1746854974, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-10", "dateLabelColor": "#0969da"}, "P7": {"htmlDir": "docs/post/MNN-LLM.html", "labels": ["MNN"], "postTitle": "MNN-LLM", "postUrl": "post/MNN-LLM.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/7", "commentNum": 0, "wordCount": 1245, "description": "\n\n```\ngit lfs install\ngit clone https://www.modelscope.cn/qwen/Qwen2-0.5B-Instruct.git\n\n\u6216\u8005\nhuggingface-cli download taobao-mnn/Qwen2-0.5B-Instruct-MNN --local-dir .//Qwen2-0.5B-Instruct-MNN\n```\n\n# export\n\n```\npip3 install torch torchvision torchaudio --force-reinstall\npip install numpy==1.26.4\npython llmexport.py --path /media/code/mnn/Qwen2-0.5B-Instruct --export mnn\n\n\n\n```\n\n\n\n# build\n```\ncmake -DMNN_BUILD_DEMO=ON -DMNN_BUILD_CONVERTER=ON -DMNN_BUILD_BENCHMARK=true -DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true ..\n\nmake -j16\n```\n\n\n\n# run\n```\n./llm_demo /media/code/mnn/MNN/transformers/llm/export/model/config.json\n```\n\n\n# issue\n\u804a\u5929\u6548\u679c\u5f88\u5dee\u3002", "top": 0, "createdAt": 1746864102, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-10", "dateLabelColor": "#0969da"}, "P8": {"htmlDir": "docs/post/Qwen3-xing-neng-ping-ce--evalscope.html", "labels": ["MNN"], "postTitle": "Qwen3\u6027\u80fd\u8bc4\u6d4b-evalscope", "postUrl": "post/Qwen3-xing-neng-ping-ce--evalscope.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/8", "commentNum": 0, "wordCount": 3035, "description": "\n\n\n\n# \u73af\u5883\u51c6\u5907 \n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA GeForce GTX 1060        Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   62C    P8               3W /  78W |   2426MiB /  6144MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A       958      G   /usr/lib/xorg/Xorg                            4MiB |\n|    0   N/A  N/A      3029      C   /usr/local/bin/ollama                      2418MiB |\n+---------------------------------------------------------------------------------------+\n```\n\n\n```\nollama run qwen3:1.7b\npip install evalscope\n```\n\n\n# \u8fd0\u884c\n```\nevalscope perf \\\n    --model qwen3:1.7b \\\n    --url 'http://192.168.1.3:11434/v1/chat/completions' \\\n    --parallel 5 \\\n    --number 20 \\\n    --api openai \\\n    --dataset openqa \\\n    --stream\n```\n\n\n# \u7ed3\u679c\n```\n'Time taken for tests (s)': 288.8133,\n'Number of concurrency': 5,\n'Total requests': 10,\n'Succeed requests': 10,\n'Failed requests': 0,\n'Output token throughput (tok/s)': 36.605,\n'Total token throughput (tok/s)': 37.5848,\n'Request throughput (req/s)': 0.0346,\n'Average latency (s)': 112.7879,\n'Average time to first token (s)': 22.1013,\n'Average time per output token (s)': 0.0865,\n'Average input tokens per request': 28.3,\n'Average output tokens per request': 1057.2,\n'Average package latency (s)': 0.0865,\n'Average package per request': 1048.3\n```\n\n\n\n\n\n\n# reference\n- [Best Practices for Evaluating the Qwen3 Model | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)\n- [\u91cf\u5316\u6a21\u578b\u6548\u679c\u8bc4\u4f30 - Qwen](https://qwen.readthedocs.io/zh-cn/latest/getting_started/quantization_benchmark.html)\n- [How Smart is Your AI? Full Assessment of IQ and EQ! | EvalScope](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html)\n- \u3002", "top": 0, "createdAt": 1746977354, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-11", "dateLabelColor": "#0969da"}, "P9": {"htmlDir": "docs/post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html", "labels": ["llama3.cpp"], "postTitle": "llama3.cpp\u57fa\u4e8eNVIDIA1060\u8fdb\u884c\u56fe\u7247\u5bf9\u8bdd", "postUrl": "post/llama3.cpp-ji-yu-NVIDIA1060-jin-xing-tu-pian-dui-hua.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/9", "commentNum": 0, "wordCount": 1422, "description": "\n# \u73af\u5883\n\u663e\u5361\uff1aNVIDIA GeForce GTX 1060\n\u56fe\u7247\u5bf9\u8bdd\u6a21\u578b\uff1aSmolVLM-500M-Instruct\n\n\n\n# \u7f16\u8bd1\n\n```\nsudo apt-get install libssl-dev libcurl4-openssl-dev\n\ngit clone https://github.com/ggml-org/llama.cpp.git\ncmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=61\nmake -j16\n```\n\n\n\u5728\u8fd9\u91cc[Ollama GPU\u652f\u6301 - Nvidia\u548cAMD GPU\u517c\u5bb9\u6027 | LlamaFactory | LlamaFactory](https://www.llamafactory.cn/ollama-docs/gpu.html#google_vignette)\u627e\u52301060\u7684\u67b6\u6784\u662f6.1\uff0c\u7f16\u8bd1\u65f6\u6307\u5b9a-DCMAKE_CUDA_ARCHITECTURES=61\n\n\n# \u6a21\u578bGGUF\u4e0b\u8f7d\n```\nexport HF_HOME=/media/dataset/hf\nexport HF_ENDPOINT=https://hf-mirror.com\nexport HUGGINGFACE_TOKEN=xxx\n\nhuggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF\nhuggingface-cli download ggml-org/SmolVLM-500M-Instruct-GGUF --local-dir .\n```\n\n\n# \u8fd0\u884cllama3.cpp\n```\nexport PATH'$PATH:/path/to/llama.cpp/build/bin\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/llama.cpp/build/bin\n\nllama-server -m ./SmolVLM-500M-Instruct-Q8_0.gguf --mmproj mmproj-SmolVLM-500M-Instruct-Q8_0.gguf --host 0.0.0.0 --port 8080 -ngl 100\n```\n\n\n\u8f93\u51fa\u65e5\u5fd7:\n```\nUser: Hello<end_of_utterance>\nAssistant: Hi there<end_of_utterance>\nUser: How are you?<end_of_utterance>\nAssistant:'\nmain: server is listening on http://0.0.0.0:8080 - starting the main loop\nsrv  update_slots: all slots are idle\n```\n\n\n- \u7528\u6d4f\u89c8\u5668\u6253\u5f00\u7f51\u5740\uff0c\u6bd4\u5982 192.168.3.100:1080\uff0c\u5373\u53ef\u4ee5\u8fdb\u884c\u56fe\u7247\u5bf9\u8bdd\u3002", "top": 0, "createdAt": 1747234848, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-14", "dateLabelColor": "#0969da"}, "P10": {"htmlDir": "docs/post/minimind.inference.html", "labels": ["minimind"], "postTitle": "minimind.inference", "postUrl": "post/minimind.inference.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/10", "commentNum": 0, "wordCount": 1870, "description": "\n# download\n```\ngit clone https://github.com/jingyaogong/minimind.git\nhuggingface-cli download --local-dir MiniMind2\n```\n\n# test\n```\npython eval_model.py --load 1 --model_mode 2\n\ncd scripts\nstreamlit run web_demo.py\n\n```\n\n# \u4f7f\u7528llama.cpp\u63a8\u7406\n\u5728llama.cpp\u4e2d\u6253\u4e0a\u5982\u4e0bpatch:\n```\ndiff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py\nindex bf6bc683..5ea853b6 100755\n--- a/convert_hf_to_gguf.py\n+++ b/convert_hf_to_gguf.py\n@@ -808,7 +808,8 @@ class TextModel(ModelBase):\n             logger.warning(f'** chkhsh:  {chkhsh}')\n             logger.warning('**************************************************************************************')\n             logger.warning('\\n')\n-            raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')\n+            res = 'smollm'\n+            #raise NotImplementedError('BPE pre-tokenizer was not recognized - update get_vocab_base_pre()')\n \n         logger.debug(f'tokenizer.ggml.pre: {repr(res)}')\n         logger.debug(f'chkhsh: {chkhsh}')\n```\n\n```\npython convert_hf_to_gguf.py ../minimind/MiniMind2/\n\n./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M\n\n./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml\n\n```\n\n# \u4f7f\u7528ollama\u90e8\u7f72\n\u65b0\u5efaminimind.modelfile\uff1a\n```\nFROM ./MiniMind2-109M-F16.gguf\nTEMPLATE '''{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n'''\n```\n\n\u52a0\u8f7d\u6a21\u578b\u5e76\u547d\u540d\u4e3aminimind2\n```\nollama create -f minimind.modelfile minimind2\n```\n\n\u542f\u52a8\u63a8\u7406\n\n```\nollama run minimind2\n```\n\n# reference\n[jingyaogong/minimind: \ud83d\ude80\ud83d\ude80 \u300c\u5927\u6a21\u578b\u300d2\u5c0f\u65f6\u5b8c\u5168\u4ece0\u8bad\u7ec326M\u7684\u5c0f\u53c2\u6570GPT\uff01\ud83c\udf0f Train a 26M-parameter GPT from scratch in just 2h!](https://github.com/jingyaogong/minimind#vllm%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1)\n\u3002", "top": 0, "createdAt": 1747542097, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-05-18", "dateLabelColor": "#0969da"}, "P11": {"htmlDir": "docs/post/Android K70 Pro llama.cpp Gemma-3n.html", "labels": ["llama3.cpp"], "postTitle": "Android K70 Pro llama.cpp Gemma-3n", "postUrl": "post/Android%20K70%20Pro%20llama.cpp%20Gemma-3n.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/11", "commentNum": 0, "wordCount": 3137, "description": "# \u8bbe\u5907\n- K70 Pro Adreno 750 (Snapdragon 8 Gen 3)\n- NDK26\n- Adreno\u4f7f\u7528opencl\u7f16\u8bd1\n\n\n# \u6a21\u578b\n- gemma-3n-E2B-it-UD-Q4_K_XL.gguf\n\n\n# Android CPU\u4e0a\u8fd0\u884c\n```\nndk-r26d:\ncmake -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-28 -DCMAKE_C_FLAGS='-march=armv8.7a' -DCMAKE_CXX_FLAGS='-march=armv8.7a' -DGGML_OPENMP=OFF  -DGGML_LLAMAFILE=OFF -DLLAMA_CURL=OFF -B build-android\ncmake --build build -j16\ncmake --install build-android --prefix ../install\n\ncd ../install\nadb shell 'mkdir /data/local/tmp/llama.cpp'\nadb push install/. /data/local/tmp/llama.cpp/\nadb push {model}.gguf /data/local/tmp/llama.cpp/\nadb shell 'ls /data/local/tmp/llama.cpp'\nadb shell\ncd /data/local/tmp/llama.cpp/lib\nexport LD_LIBRARY_PATH=/data/local/tmp/llama.cpp/lib:$LD_LIBRARY_PATH\n./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose\n```\n\n\n# Adreno\u4e0a\u8fd0\u884c\n```\nAdreno:\n\nInstall OpenCL Headers and Library\ngit clone https://github.com/KhronosGroup/OpenCL-Headers && \\\ncd OpenCL-Headers && \\\ncp -rf ./CL/ $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include\n\ngit clone https://github.com/KhronosGroup/OpenCL-ICD-Loader && \\\ncd OpenCL-ICD-Loader && \\\nmkdir build_ndk26 && cd build_ndk26\ncmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DOPENCL_ICD_LOADER_HEADERS_DIR=$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/include -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=24 -DANDROID_STL=c++_shared\ncp libOpenCL.so $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android\n\nBuild llama.cpp:\ncmake .. -G Ninja -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a  -DANDROID_PLATFORM=android-28  -DBUILD_SHARED_LIBS=OFF -DGGML_OPENCL=ON  -DLLAMA_CURL=OFF\nninja -j16\nDESTDIR=../install-opencl/ ninja install\n\n\ncd ../install-opencl/usr/loca/\nadb shell 'mkdir /data/local/tmp/llama.cpp-opencl'\nadb push . /data/local/tmp/llama.cpp-opencl/\nadb push {model}.gguf /data/local/tmp/llama.cpp-opencl/\nadb shell 'ls /data/local/tmp/llama.cpp-opencl'\nadb shell\ncd /data/local/tmp/llama.cpp-opencl/lib\nexport LD_LIBRARY_PATH=/data/local/tmp/llama.cpp-opencl/lib:$LD_LIBRARY_PATH\n./llama-server -m ../gemma-3n-E2B-it-UD-Q4_K_XL.gguf --top-p 0.95 --temp 0.7 --frequency-penalty 0 --presence-penalty 0 -n 40960 -s -1 --dynatemp-range 0 --dynatemp-exp 1 --top-k 40 --min-p 0.05 --typical 1 --repeat-last-n 64 --repeat-penalty 1 --mirostat 0 --mirostat-ent 5 --mirostat-lr 0.1  -c 40960 -np 1 -t -1 -ngl 14 --host 0.0.0.0 --verbose\n```\n\n\n# Reference\n- https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n- https://github.com/ggml-org/llama.cpp/blob/master/docs/backend/OPENCL.md\n- [google-ai-edge](https://github.com/google-ai-edge)\n\n# issue\n- \u7528CPU\u57fa\u672c\u4e0eGoogle\u5b98\u65b9\u7684gallery\u6027\u80fd\u4e00\u81f4\u3002", "top": 0, "createdAt": 1751188577, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-06-29", "dateLabelColor": "#0969da"}, "P12": {"htmlDir": "docs/post/0713.Xiaomi14Ultra-tong-guo-QNN-yun-xing-Qwen.html", "labels": ["QNN"], "postTitle": "0713.Xiaomi14Ultra\u901a\u8fc7QNN\u8fd0\u884cQwen", "postUrl": "post/0713.Xiaomi14Ultra-tong-guo-QNN-yun-xing-Qwen.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/12", "commentNum": 0, "wordCount": 4859, "description": "\n# environment\n- Operating System: Ubuntu 22.04\n- QNN SDK Version: 2.31.0.250130\n- Hexagon SDK Version: 5.5.5.0\n- Android NDK Version: android-ndk-r26d\n- devices\uff1axiaomi 14Ultra, 8gen3\n- mllm\n\n\n\n# build\n```\ncd /opt/qcom/Hexagon_SDK\nln -s 5.5.5.0 HexagonSDK\n\nexport QNN_SDK_ROOT=/opt/qcom/aistack/qairt/2.31.0.250130\nsource $QNN_SDK_ROOT/bin/envsetup.sh\nsource /opt/qcom/Hexagon_SDK/5.5.5.0/setup_sdk_env.source\n\ngit clone https://github.com/UbiquitousLearning/mllm.git\n\nCMakeLists.txt \u4e2dgtest\u76f8\u5173\u7684\u6ce8\u91ca\u6389\n\n\n\ncd src/backends/qnn\nln -s $QNN_SDK_ROOT sdk\n\ncd src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/\nmake htp_aarch64 && make htp_v75\n\n./build_android_qnn.sh\n./run_qwen_qnn.sh\n\n\u8981\u52a0\u4e0b\u8fd9\u4e24\u53e5:\nadb shell mkdir -p /data/local/tmp/mllm/bin\nadb push ../vocab/qwen_merges.txt /data/local/tmp/mllm/vocab/\n```\n\n\n# result\n```\n./run_qwen_qnn.sh \n../vocab/qwen_vocab.mllm: 1 file pushed, 0 skipped. 95.0 MB/s (3189278 bytes in 0.032s)\n../vocab/qwen_merges.txt: 1 file pushed, 0 skipped. 81.8 MB/s (1671839 bytes in 0.019s)\nqwen-1.5-1.8b-chat-int8 file already exists\nqwen-1.5-1.8b-chat-q4k.mllm file already exists\nQNN_SDK_ROOT is set to /opt/qcom/aistack/qairt/2.31.0.250130\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtp.so: 1 file pushed, 0 skipped. 53.6 MB/s (2060792 bytes in 0.037s)\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpV75Stub.so: 1 file pushed, 0 skipped. 22.9 MB/s (453640 bytes in 0.019s)\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpPrepare.so: 1 file pushed, 0 skipped. 33.4 MB/s (55080032 bytes in 1.575s)\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpProfilingReader.so: 1 file pushed, 0 skipped. 113.0 MB/s (579072 bytes in 0.005s)\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpOptraceProfilingReader.so: 1 file pushed, 0 skipped. 153.1 MB/s (3073968 bytes in 0.019s)\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/aarch64-android/libQnnHtpV75CalculatorStub.so: 1 file pushed, 0 skipped. 0.4 MB/s (6472 bytes in 0.015s)\n/opt/qcom/aistack/qairt/2.31.0.250130/lib/hexagon-v75/unsigned/libQnnHtpV75Skel.so: 1 file pushed, 0 skipped. 89.1 MB/s (8485728 bytes in 0.091s)\n../src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/build/aarch64-android/libQnnLLaMAPackage.so: 1 file pushed, 0 skipped. 300.3 MB/s (1479704 bytes in 0.005s)\n../src/backends/qnn/LLaMAOpPackageHtp/LLaMAPackage/build/hexagon-v75/libQnnLLaMAPackage.so: 1 file pushed, 0 skipped. 177.0 MB/s (818800 bytes in 0.004s)\n../bin-arm/demo_qwen_npu: 1 file pushed, 0 skipped. 72.1 MB/s (93400608 bytes in 1.235s)\n[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:118] Backend: libQnnHtp.so\n[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:143] Backend        build version: v2.31.0.250130151446_114721\n[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:166] Initialize Backend Returned Status = 0\n[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:636] Registered Op Package: libQnnLLaMAPackage_CPU.so and interface provider: LLaMAPackageInterfaceProvider\n[INFO] Sun Jul 13 14:43:12 2025 [/home/liuqi/code/llm/genie/mllm/src/backends/qnn/QNNBackend.cpp:636] Registered Op Package: libQnnLLaMAPackage_HTP.so and interface provider: LLaMAPackageInterfaceProvider\nWarmup finished.\n[Q] 'Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. These models are trained on vast amounts of data, enabling them to perform a wide range of tasks, from answering questions and summarizing text to generating creative content and engaging in conversational dialogue. LLMs like GPT-3 and GPT-4, developed by OpenAI, have set new benchmarks in natural language processing by leveraging deep learning architectures, particularly transformer models, which excel at capturing context and relationships within text. The scalability and versatility of LLMs make them invaluable tools for applications in education, customer service, content creation, and more. However, their deployment also raises ethical considerations, including issues of bias, misinformation, and the potential for misuse. As the field continues to evolve, ongoing research and responsible deployment strategies are essential to harnessing the full potential of these powerful AI systems while mitigating their risks.'\nGenerate a title based on the above text.\n[A] Exploring the Advantages and Ethical Considerations of Large Language Models (LLMs): A Comprehensive OverviewSegmentation fault \n```\n\n\n# issue\n- \u901f\u5ea6\u5f88\u6162\uff0c\u4e0d\u4e86\u89e3\u539f\u56e0\n\n\n# references\n- [mllm/src/backends/qnn/README.md at main \u00b7 UbiquitousLearning/mllm](https://github.com/UbiquitousLearning/mllm/blob/main/src/backends/qnn/README.md)\n- \n\u3002", "top": 0, "createdAt": 1752390298, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-07-13", "dateLabelColor": "#0969da"}, "P13": {"htmlDir": "docs/post/xiao-mi-15Ultra-yun-xing-Qwen2-7B-Instruct.html", "labels": ["QNN"], "postTitle": "\u5c0f\u7c7315Ultra\u8fd0\u884cQwen2-7B-Instruct", "postUrl": "post/xiao-mi-15Ultra-yun-xing-Qwen2-7B-Instruct.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/13", "commentNum": 0, "wordCount": 5903, "description": "\n# environment\n- Operating System: Ubuntu 22.04\n- QNN SDK Version: 2.36.0.250627\n- devices\uff1axiaomi 15Ultra 16G\u5185\u5b58, Snapdragon\u00ae 8 Elite Mobile\n- genie-t2t-run\n\n- \u6ce8\u610f\uff0c[Qwen2-7B-Instruct - Qualcomm AI Hub](https://aihub.qualcomm.com/models/qwen2_7b_instruct)\u76ee\u524d\u53ea\u652f\u6301Snapdragon\u00ae 8 Elite Mobile\u3002", "top": 0, "createdAt": 1752588217, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-07-15", "dateLabelColor": "#0969da"}, "P14": {"htmlDir": "docs/post/Redmi K70 Pro-yun-xing-Llama 1.1B.html", "labels": ["QNN"], "postTitle": "Redmi K70 Pro\u8fd0\u884cLlama 1.1B", "postUrl": "post/Redmi%20K70%20Pro-yun-xing-Llama%201.1B.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/14", "commentNum": 0, "wordCount": 3897, "description": "\n# \u524d\u8a00\n\n\u968f\u7740\u79fb\u52a8\u8bbe\u5907\u786c\u4ef6\u6027\u80fd\u7684\u98de\u901f\u53d1\u5c55\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u90e8\u7f72\u5230\u7aef\u4fa7\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684AI\u63a8\u7406\uff0c\u6b63\u6210\u4e3a\u4e00\u4e2a\u70ed\u95e8\u65b9\u5411\u3002", "top": 0, "createdAt": 1752766270, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-07-17", "dateLabelColor": "#0969da"}, "P15": {"htmlDir": "docs/post/HexagonNPU-yun-xing-llama.cpp.html", "labels": ["QNN"], "postTitle": "HexagonNPU\u8fd0\u884cllama.cpp", "postUrl": "post/HexagonNPU-yun-xing-llama.cpp.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/15", "commentNum": 0, "wordCount": 7470, "description": "\u672c\u6587\u5c06\u8be6\u7ec6\u4ecby\u7ecd\u5982\u4f55\u5728 Android \u8bbe\u5907\u4e0a\uff0c\u5229\u7528 `llama.cpp` \u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u901a\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5355\u5143 (QNN) \u7684\u786c\u4ef6\u52a0\u901f\u80fd\u529b\uff0c\u90e8\u7f72\u5e76\u8fd0\u884c Qwen 1.5 \u6a21\u578b\u3002", "top": 0, "createdAt": 1753189317, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-07-22", "dateLabelColor": "#0969da"}, "P16": {"htmlDir": "docs/post/xiao-mi-15 NPU-bu-shu-Llama-3.2-1B-Instruct.html", "labels": ["QNN"], "postTitle": "\u5c0f\u7c7315 NPU\u90e8\u7f72Llama-3.2-1B-Instruct", "postUrl": "post/xiao-mi-15%20NPU-bu-shu-Llama-3.2-1B-Instruct.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/16", "commentNum": 0, "wordCount": 7225, "description": "# \u51c6\u5907\n- https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n- \u663e\u5b58\u81f3\u5c11\u9700\u898112G\u3002", "top": 0, "createdAt": 1754219210, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-08-03", "dateLabelColor": "#0969da"}, "P17": {"htmlDir": "docs/post/d2ltvm-bian-yi.html", "labels": ["TVM"], "postTitle": "d2ltvm\u7f16\u8bd1", "postUrl": "post/d2ltvm-bian-yi.html", "postSourceUrl": "https://github.com/Ethan-a2/Ethan-a2.github.io/issues/17", "commentNum": 0, "wordCount": 2931, "description": "\n# \u73af\u5883\n- tvm-0.19 + numpy  < 1.24\n- ubuntu 22.04\n- llvm-15\n- \u56e0\u4e3a\u4f9d\u8d56mxnet\uff0c\u4f46\u662fmxnet 2022\u540e\u4e0d\u518d\u66f4\u65b0\uff0c\u6240\u4ee5numpy\u8981\u964d\u7ea7\u52301.24\u4ee5\u524d\u7684\u7248\u672c\n- TVM 0.20\u7684\u7248\u672c\u6709\u4e86\u6bd4\u8f83\u5927\u7684\u66f4\u65b0\uff0c\u6240\u4ee5\u8fd9\u91cc\u9002\u914d\u4e860.19\u7684\u7248\u672c\n\t- https://github.com/apache/tvm/issues/17914\n- LLVM\u66f4\u9ad8\u7684\u7248\u672c\u5e94\u8be5\u4e5f\u662f\uff0c\u4f46\u662fubuntu22.04\u5b89\u88c5llvm-15\u6bd4\u8f83\u65b9\u4fbf\uff0c\u5c31\u5148\u7528\u4e86\u8fd9\u4e2a\u7248\u672c\n\n\n# \u7f16\u8bd1\u901a\u8fc7\u7684\u7248\u672c\n- https://github.com/Ethan-a2/d2ltvm\n- \u9002\u914d\u4e86TVM0.19\u4e4b\u540e\u7684\u7248\u672c\uff0c\u524d4\u7ae0\u53ef\u4ee5\u6b63\u5e38\u7f16\u8bd1\u8fd0\u884c\n\n# \u5404\u7ae0\u601d\u7ef4\u5bfc\u56fe\n\n\n```mermaid\nmindmap\n  root((Getting Started))\n    Installation\n      Python env\n      TVM build (LLVM, CUDA)\n      MXNet\n    Vector Add\n      NumPy impl\n      TVM compute\n      Schedule\n      Build & Run\n    Neural Network Inference\n      Pretrained model (ResNet18)\n      Relay IR\n      Compilation (LLVM)\n      Save/Load Module\n    Remote Execution\n      RPC server\n      Cross-compilation\n      Run on remote device\n```\n\n\n\n\n\n```mermaid\nmindmap\n  root((Expressions for Operators))\n    Data Types\n      float32\n      int8\n      cast/astype\n    Shapes\n      te.var\n      multi-dim tensors\n    Index & Shape Expressions\n      Transpose\n      Reshape\n      Slicing\n    Reduction\n      te.reduce_axis\n      te.sum, te.max, te.min\n      comm_reducer\n    Conditional\n      if_then_else\n    Truth Values\n      all\n      any\n```\n\n\n\n```mermaid\nmindmap\n  root((Common Operators))\n    Broadcast Add\n    MatMul\n    Conv2D\n    Depthwise Conv\n    Pooling\n      MaxPool\n      AvgPool\n    BatchNorm\n```\n\n\n\n```mermaid\nmindmap\n  root((CPU Operator Optimizations))\n    CPU Architecture\n      Cache\n      SIMD\n      Pipeline\n    Function Call Overhead\n    Vector Add\n    Broadcast Add\n    MatMul\n      Loop blocking\n      Vectorization\n    Convolution\n      Packed layout\n    Pooling\n    BatchNorm\n```\n\n\n\n```mermaid\nmindmap\n  root((GPU Operator Optimizations))\n    GPU Architecture\n      Warp\n      SM\n      Shared Memory\n    Vector Add\n    Broadcast Add\n    MatMul\n      Thread tiling\n      Shared memory\n    Conv2D\n    Depthwise Conv\n    Pooling\n    BatchNorm\n```\n\n\n\n# TVM\u7f16\u8bd1\n\n```\nsudo apt-get install -y -q \\\n        git \\\n        cmake \\\n        build-essential \\\n        libtinfo-dev \\\n        libffi-dev \\\n        zlib1g-dev \\\n        llvm-15-dev \\\n        libclang-15-dev \\\n        ninja-build\n\nsudo apt install nvidia-cuda-toolkit\n\ngit clone --recursive https://github.com/apache/tvm tvm-src\nmkdir build; cd build\n\ncp ../cmake/config.cmake .\necho 'set(USE_LLVM \\'llvm-config-15 --ignore-libllvm --link-static\\')' >> config.cmake\nsed -i 's/# set(USE_CUDA OFF)/set(USE_CUDA ON)/g' config.cmake\nset(USE_GTEST OFF)\n\ncmake ../\ncmake --build . --parallel $(nproc)\n```\n\n\n# d2ltvm\u76f8\u5173\u624b\u518c\n- https://tvm.d2l.ai/d2l-tvm.pdf\n- https://github.com/d2l-ai/d2l-tvm\n- http://tvm.d2l.ai/d2l-tvm.zip\n- https://tvm.d2l.ai/\n\n\n# \u7ed3\u8bba\n- \u63d0\u4f9b\u4e00\u79cd\u65b9\u5f0f\uff1a\u4e0d\u7528\u624b\u5de5\u5199\u5e95\u5c42 CUDA/C++ \u5185\u6838\uff0c\u800c\u662f\u901a\u8fc7\u7f16\u8bd1\u5668\u6846\u67b6\u7edf\u4e00\u8868\u8fbe\u548c\u4f18\u5316\u7b97\u5b50/\u6a21\u578b\u3002", "top": 0, "createdAt": 1757236148, "style": "", "script": "", "head": "", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "createdDate": "2025-09-07", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"bug": "#d73a4a", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "llama3.cpp": "#e99695", "minimind": "#c2e0c6", "MNN": "#5319e7", "QNN": "#b90ebe", "question": "#d876e3", "TVM": "#4f5f8a", "wontfix": "#ffffff"}, "displayTitle": "Brue Leo's Blog", "faviconUrl": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "ogImage": "https://avatars.githubusercontent.com/u/61342241?v=4&size=64", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://Ethan-a2.github.io", "prevUrl": "/index.html", "nextUrl": "disabled"}